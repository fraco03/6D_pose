{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Clone or pull part\n",
    "repo_url = \"https://github.com/fraco03/6D_pose.git\"\n",
    "repo_dir = \"/content/6D_pose\"\n",
    "branch = \"pose_rgb\"\n",
    "\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone -b {branch} {repo_url}\n",
    "    print(f\"Cloned {repo_url}\")\n",
    "else:\n",
    "    %cd {repo_dir}\n",
    "    !git fetch origin\n",
    "    !git checkout {branch}\n",
    "    !git reset --hard origin/{branch}\n",
    "    %cd ..\n",
    "    print(f\"Updated {repo_url}\")\n",
    "\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.insert(0, repo_dir)\n",
    "\n",
    "%cd 6D_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209858ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from utils.load_data import mount_drive\n",
    "\n",
    "mount_drive()\n",
    "\n",
    "dataset_root = \"/content/drive/MyDrive/Linemod_preprocessed\"\n",
    "print(f\"\\nâœ… Setup complete!\")\n",
    "print(f\"ðŸ“ Dataset path: {dataset_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae81c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pose_rgb.pointcloud_dataset import LineModPointCloudDataset\n",
    "from src.pose_rgb.pointnet_model import PointNetPose\n",
    "from src.pose_rgb.loss import AutomaticWeightedLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ðŸ”¥ Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff242eb0",
   "metadata": {},
   "source": [
    "## ðŸ“ Note sulla Loss Function\n",
    "\n",
    "**Differenza tra PointNet e RGB approach:**\n",
    "\n",
    "- **RGB (ResNet + TranslationNet)**: \n",
    "  - Predice `[dx, dy, log(z)]` â†’ poi pinhole projection â†’ `[X, Y, Z]`\n",
    "  - Usa `DisentangledTranslationLoss`: separa XY da Z\n",
    "  - Ha senso perchÃ© XY dipendono dalla geometria pinhole, Z Ã¨ indipendente\n",
    "\n",
    "- **PointNet**:\n",
    "  - Predice **direttamente** `[X, Y, Z]` dalla point cloud\n",
    "  - Usa loss **unificata** per translation: tratta X, Y, Z simmetricamente\n",
    "  - Non c'Ã¨ pinhole projection, quindi non ha senso separare XY da Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ac005",
   "metadata": {},
   "source": [
    "## âš¡ Performance Optimizations\n",
    "\n",
    "**Ottimizzazioni applicate per velocizzare il training:**\n",
    "\n",
    "1. **Riduzione punti**: 1024 â†’ 512 punti per point cloud (~2x speed-up)\n",
    "2. **Cached intrinsics**: Usa `linemod_config` invece di caricare YAML ogni volta (~2x speed-up)\n",
    "3. **Torch sampling**: `torch.randperm` invece di `np.random.choice` (~1.5x speed-up)\n",
    "4. **Mixed precision**: FP16/FP32 automatico con `torch.cuda.amp` (~1.5x speed-up)\n",
    "5. **Batch size**: Aumentato da 32 â†’ 64 per migliore GPU utilization\n",
    "6. **DataLoader**: `num_workers=4` + `pin_memory=True` per I/O parallelo\n",
    "\n",
    "**Speed-up totale stimato: ~5-8x** rispetto alla versione iniziale! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9722028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea dataset con point clouds\n",
    "# use_rgb=True -> point cloud con 6 canali [x,y,z,r,g,b]\n",
    "# use_rgb=False -> point cloud con 3 canali [x,y,z]\n",
    "\n",
    "train_dataset = LineModPointCloudDataset(\n",
    "    root_dir=dataset_root,\n",
    "    split='train',\n",
    "    num_points=512,  # Ridotto per speed-up (era 1024)\n",
    "    use_rgb=True      # Include RGB\n",
    ")\n",
    "\n",
    "test_dataset = LineModPointCloudDataset(\n",
    "    root_dir=dataset_root,\n",
    "    split='test',\n",
    "    num_points=512,\n",
    "    use_rgb=True\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza un sample\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "print(f\"Point cloud shape: {sample['point_cloud'].shape}\")  # (1024, 6)\n",
    "print(f\"Rotation shape: {sample['rotation'].shape}\")        # (4,)\n",
    "print(f\"Translation shape: {sample['translation'].shape}\")  # (3,)\n",
    "print(f\"\\nRotation (quat): {sample['rotation']}\")\n",
    "print(f\"Translation (m): {sample['translation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders - Ottimizzati per performance\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64,  # Aumentato da 32 per migliore GPU utilization\n",
    "    shuffle=True, \n",
    "    num_workers=4,  # Aumentato da 2\n",
    "    pin_memory=True  # Velocizza transfer CPU->GPU\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf4309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# ==========================================\n",
    "# HYPERPARAMETERS\n",
    "# ==========================================\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50\n",
    "USE_MIXED_PRECISION = True  # Mixed precision per speed-up ~1.5x\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "CHECKPOINT_DIR = f'/content/drive/MyDrive/runs/pointnet_{timestamp}'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize PointNet Model\n",
    "# input_channels=6 perchÃ© usiamo [x,y,z,r,g,b]\n",
    "model = PointNetPose(input_channels=6, use_batch_norm=True).to(DEVICE)\n",
    "\n",
    "# Loss per PointNet: use_disentangled=False perchÃ© non usiamo pinhole projection\n",
    "# Translation viene predetta direttamente come [X,Y,Z] simmetrico\n",
    "criterion = AutomaticWeightedLoss(use_disentangled=False).to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(criterion.parameters()),\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Mixed Precision Scaler\n",
    "scaler = GradScaler(DEVICE) if USE_MIXED_PRECISION else None\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "print(f\"\\nðŸ”¥ STARTING POINTNET TRAINING on {DEVICE}...\")\n",
    "print(f\"ðŸ“ Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"âš™ï¸  Loss mode: Unified Translation (no XY/Z separation)\")\n",
    "print(f\"âš¡ Mixed Precision: {USE_MIXED_PRECISION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ef393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TRAINING LOOP (con Mixed Precision)\n",
    "# ==========================================\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # --- TRAIN PHASE ---\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        # Move to device\n",
    "        point_clouds = batch['point_cloud'].to(DEVICE, non_blocking=True)  # (B, N, 6)\n",
    "        gt_rot = batch['rotation'].to(DEVICE, non_blocking=True)           # (B, 4)\n",
    "        gt_trans = batch['translation'].to(DEVICE, non_blocking=True)      # (B, 3) in metri\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed Precision Forward + Backward\n",
    "        if USE_MIXED_PRECISION:\n",
    "            with autocast(DEVICE):\n",
    "                pred_rot, pred_trans = model(point_clouds)\n",
    "                loss, l_r, l_t, _ = criterion(pred_rot, gt_rot, pred_trans, gt_trans)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            pred_rot, pred_trans = model(point_clouds)\n",
    "            loss, l_r, l_t, _ = criterion(pred_rot, gt_rot, pred_trans, gt_trans)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_train_loss += loss.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'L_Tot': f\"{loss.item():.2f}\",\n",
    "            'L_Rot': f\"{l_r.item():.2f}\",\n",
    "            'L_Trans': f\"{l_t.item():.3f}\"\n",
    "        })\n",
    "    \n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # --- VALIDATION PHASE ---\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    val_batches_limit = 50\n",
    "    count_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_iterator = islice(test_loader, val_batches_limit)\n",
    "        val_pbar = tqdm(val_iterator, total=val_batches_limit, desc=\"Validating\")\n",
    "        \n",
    "        for batch in val_pbar:\n",
    "            point_clouds = batch['point_cloud'].to(DEVICE, non_blocking=True)\n",
    "            gt_rot = batch['rotation'].to(DEVICE, non_blocking=True)\n",
    "            gt_trans = batch['translation'].to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            if USE_MIXED_PRECISION:\n",
    "                with autocast(DEVICE):\n",
    "                    pred_rot, pred_trans = model(point_clouds)\n",
    "                    loss, _, _, _ = criterion(pred_rot, gt_rot, pred_trans, gt_trans)\n",
    "            else:\n",
    "                pred_rot, pred_trans = model(point_clouds)\n",
    "                loss, _, _, _ = criterion(pred_rot, gt_rot, pred_trans, gt_trans)\n",
    "            \n",
    "            running_val_loss += loss.item()\n",
    "            count_batches += 1\n",
    "    \n",
    "    avg_val_loss = running_val_loss / count_batches if count_batches > 0 else 0.0\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # --- REPORT & SAVE ---\n",
    "    print(f\"ðŸ“Š Epoch {epoch+1}: Train={avg_train_loss:.4f} | Val={avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'criterion_state_dict': criterion.state_dict(),\n",
    "            'val_loss': best_val_loss\n",
    "        }, os.path.join(CHECKPOINT_DIR, \"best_model.pth\"))\n",
    "        \n",
    "        print(f\"ðŸ† New Best Model! (Loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    # Save last checkpoint\n",
    "    if (epoch + 1) == NUM_EPOCHS:\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'criterion_state_dict': criterion.state_dict(),\n",
    "        }, os.path.join(CHECKPOINT_DIR, f\"checkpoint_ep{epoch+1}.pth\"))\n",
    "\n",
    "print(\"\\nðŸŽ‰ TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', marker='o', alpha=0.7)\n",
    "plt.plot(val_losses, label='Validation Loss', marker='s', alpha=0.7)\n",
    "if best_epoch > 0:\n",
    "    plt.axvline(x=best_epoch-1, color='r', linestyle='--', alpha=0.5, \n",
    "                label=f'Best Epoch ({best_epoch})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('PointNet Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Train Loss', marker='o', alpha=0.7)\n",
    "plt.plot(val_losses, label='Validation Loss', marker='s', alpha=0.7)\n",
    "if best_epoch > 0:\n",
    "    plt.axvline(x=best_epoch-1, color='r', linestyle='--', alpha=0.5, \n",
    "                label=f'Best Epoch ({best_epoch})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.yscale('log')\n",
    "plt.title('PointNet Training History (Log)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, 'training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Training Statistics:\")\n",
    "print(f\"   Best epoch: {best_epoch}\")\n",
    "print(f\"   Best val loss: {best_val_loss:.6f}\")\n",
    "print(f\"   Final train loss: {train_losses[-1]:.6f}\")\n",
    "\n",
    "# Save history\n",
    "history = {\n",
    "    'train_losses': [float(x) for x in train_losses],\n",
    "    'val_losses': [float(x) for x in val_losses],\n",
    "    'best_epoch': int(best_epoch),\n",
    "    'best_val_loss': float(best_val_loss),\n",
    "    'timestamp': timestamp\n",
    "}\n",
    "\n",
    "with open(os.path.join(CHECKPOINT_DIR, 'history.json'), 'w') as f:\n",
    "    json.dump(history, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc8bf1c",
   "metadata": {},
   "source": [
    "## Visualize Point Cloud Sample\n",
    "\n",
    "Visualizziamo una point cloud dal dataset per verificare che tutto funzioni correttamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e081a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Visualizza una point cloud\n",
    "sample = train_dataset[100]\n",
    "pc = sample['point_cloud'].numpy()  # (1024, 6) [x, y, z, r, g, b]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot 3D points con colori RGB\n",
    "ax.scatter(pc[:, 0], pc[:, 1], pc[:, 2], \n",
    "           c=pc[:, 3:6], s=2, alpha=0.6)\n",
    "\n",
    "ax.set_xlabel('X (m)')\n",
    "ax.set_ylabel('Y (m)')\n",
    "ax.set_zlabel('Z (m)')\n",
    "ax.set_title(f'Point Cloud - Object {sample[\"object_id\"]}')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Object ID: {sample['object_id']}\")\n",
    "print(f\"Rotation (quat): {sample['rotation']}\")\n",
    "print(f\"Translation (m): {sample['translation']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
