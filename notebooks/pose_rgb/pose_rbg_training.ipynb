{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0oAXrKjNq32c",
    "outputId": "18b06df5-2594-43f9-dade-e6187644f8c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '6D_pose'...\n",
      "remote: Enumerating objects: 174, done.\u001b[K\n",
      "remote: Counting objects: 100% (174/174), done.\u001b[K\n",
      "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
      "remote: Total 174 (delta 87), reused 113 (delta 45), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (174/174), 2.23 MiB | 6.95 MiB/s, done.\n",
      "Resolving deltas: 100% (87/87), done.\n",
      "Cloned https://github.com/fraco03/6D_pose.git to /content/6D_pose\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown -q\n",
    "print(\"Downloading folder from Drive...\")\n",
    "# Downloads the folder structure containing the Linemod dataset\n",
    "!gdown \"https://drive.google.com/file/d/1Zwh-gAk_-CBgpOcNLPLdFNxggi3NTh-S/view?usp=drive_link\" --fuzzy\n",
    "import glob\n",
    "zip_files = glob.glob(\"**/Linemod_preprocessed.zip\", recursive=True)\n",
    "\n",
    "if zip_files:\n",
    "    zip_path = zip_files[0]\n",
    "    print(f\"Unzipping {zip_path}...\")\n",
    "    !unzip -q -o \"{zip_path}\"\n",
    "    print(\"Extraction complete!\")\n",
    "else:\n",
    "    print(\"Error: Linemod_preprocessed.zip not found. Check the download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T10:14:21.079678Z",
     "iopub.status.busy": "2025-12-22T10:14:21.079384Z",
     "iopub.status.idle": "2025-12-22T10:14:22.302027Z",
     "shell.execute_reply": "2025-12-22T10:14:22.301101Z",
     "shell.execute_reply.started": "2025-12-22T10:14:21.079651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '6D_pose'...\n",
      "remote: Enumerating objects: 745, done.\u001b[K\n",
      "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
      "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
      "remote: Total 745 (delta 27), reused 39 (delta 15), pack-reused 674 (from 1)\u001b[K\n",
      "Receiving objects: 100% (745/745), 11.19 MiB | 25.92 MiB/s, done.\n",
      "Resolving deltas: 100% (409/409), done.\n",
      "Cloned https://github.com/fraco03/6D_pose.git to /kaggle/working/6D_pose\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Clone or pull part\n",
    "repo_url = \"https://github.com/fraco03/6D_pose.git\"\n",
    "repo_dir = \"/kaggle/working/6D_pose\"   #Modify here for kaggle\n",
    "branch = \"main\"\n",
    "\n",
    "# Clone if missing\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone -b {branch} {repo_url}\n",
    "    print(f\"Cloned {repo_url} to {repo_dir}\")\n",
    "else:\n",
    "    %cd {repo_dir}\n",
    "    !git fetch origin\n",
    "    !git checkout {branch}\n",
    "    !git reset --hard origin/{branch}\n",
    "    %cd ..\n",
    "    print(f\"Updated {repo_url} to {repo_dir}\")\n",
    "\n",
    "# Add repository to Python path\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.insert(0, repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T10:14:24.781460Z",
     "iopub.status.busy": "2025-12-22T10:14:24.781160Z",
     "iopub.status.idle": "2025-12-22T10:14:24.902362Z",
     "shell.execute_reply": "2025-12-22T10:14:24.901623Z",
     "shell.execute_reply.started": "2025-12-22T10:14:24.781430Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—‘ï¸ Cache pulita dal disco.\n"
     ]
    }
   ],
   "source": [
    "# Cancella tutte le cartelle __pycache__ ricorsivamente nella directory di lavoro\n",
    "!find . -name \"__pycache__\" -type d -exec rm -rf {} +\n",
    "print(\"ðŸ—‘ï¸ Cache pulita dal disco.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T10:14:26.373130Z",
     "iopub.status.busy": "2025-12-22T10:14:26.372435Z",
     "iopub.status.idle": "2025-12-22T10:14:38.230650Z",
     "shell.execute_reply": "2025-12-22T10:14:38.229769Z",
     "shell.execute_reply.started": "2025-12-22T10:14:26.373093Z"
    },
    "id": "Ea5j56JR7NjI",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plyfile in c:\\users\\fcaro\\.virtualenvs\\mlvenv\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\fcaro\\.virtualenvs\\mlvenv\\lib\\site-packages (from plyfile) (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install plyfile\n",
    "from src.pose_rgb.dataset import LineModPoseDataset\n",
    "from src.pose_rgb.model import ResNetRotation, TranslationNet\n",
    "from src.pose_rgb.pose_utils import quaternion_to_rotation_matrix, convert_rotation_to_quaternion, inverse_pinhole_projection\n",
    "from src.pose_rgb.test_dataset import *\n",
    "from src.pose_rgb.loss import CombinedPoseLoss, MultiObjectPointMatchingLoss, TranslationLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pathlib\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from utils.projection_utils import *\n",
    "from utils.linemod_config import *\n",
    "from metrics import compute_ADD_metric_quaternion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-22T10:27:45.939392Z",
     "iopub.status.busy": "2025-12-22T10:27:45.938389Z",
     "iopub.status.idle": "2025-12-22T10:27:55.230588Z",
     "shell.execute_reply": "2025-12-22T10:27:55.229924Z",
     "shell.execute_reply.started": "2025-12-22T10:27:45.939340Z"
    },
    "id": "YrOmeFj_sRID",
    "outputId": "d3a4e969-a16f-4f10-ac1a-e0ffe7eb5e69",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LineModConfig initialized: F:\\Magistrale\\Advanced Machine Learning\\6D_pose\\Linemod_preprocessed_small\n",
      " Loaded LineModPoseDataset\n",
      "   Split: train (Ratio: 0.80)\n",
      "   Objects: [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "   Total samples: 147\n",
      " Loaded LineModPoseDataset\n",
      "   Split: test (Ratio: 0.20)\n",
      "   Objects: [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "   Total samples: 60\n"
     ]
    }
   ],
   "source": [
    "# root_dir = '/kaggle/input/line-mode/Linemod_preprocessed' #Modify here for kaggle\n",
    "root_dir = \"../../Linemod_preprocessed_small\"  # Adjust path as needed\n",
    "dayaset_root = root_dir\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = LineModPoseDataset(split='train', root_dir=root_dir)\n",
    "test_dataset = LineModPoseDataset(split='test', root_dir=root_dir)\n",
    "\n",
    "#Dataloder\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T10:15:49.923544Z",
     "iopub.status.busy": "2025-12-22T10:15:49.922872Z",
     "iopub.status.idle": "2025-12-22T10:15:49.933250Z",
     "shell.execute_reply": "2025-12-22T10:15:49.932332Z",
     "shell.execute_reply.started": "2025-12-22T10:15:49.923513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_all_object_points(num_points=1000):\n",
    "    \"\"\"\n",
    "    Loads .ply files for ALL objects and stacks them into a single Tensor.\n",
    "    \n",
    "    Args:\n",
    "        models_dir (str): Folder containing .ply files (e.g., 'obj_01.ply').\n",
    "        valid_obj_ids (list): List of integers IDs (e.g., [1, 5, 6...]).\n",
    "        num_points (int): Number of points to sample per object.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Shape (Num_Classes, num_points, 3).\n",
    "                      The index in dimension 0 corresponds to the index in valid_obj_ids.\n",
    "    \"\"\"\n",
    "    global dataset_root\n",
    "    linemod_config = get_linemod_config(dataset_root)\n",
    "\n",
    "    all_model_points = []\n",
    "    VALID_OBJ_IDS = [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15] \n",
    "    for obj_id in VALID_OBJ_IDS:\n",
    "        model_points = linemod_config.get_model_3d(obj_id, unit='m')  # (N, 3)\n",
    "        if model_points.shape[0] >= num_points:\n",
    "            choice = np.random.choice(model_points.shape[0], num_points, replace=False)\n",
    "        else:\n",
    "            choice = np.random.choice(model_points.shape[0], num_points, replace=True)\n",
    "        model_points = model_points[choice, :]\n",
    "        all_model_points.append(torch.tensor(model_points, dtype=torch.float32))\n",
    "    all_model_points = torch.stack(all_model_points, dim=0)  # (Num_Classes, NUM_POINTS, 3)\n",
    "    all_model_points = all_model_points.to(device)\n",
    "    return all_model_points\n",
    "\n",
    "VALID_OBJ_IDS = [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15] \n",
    "max_obj_id = max(VALID_OBJ_IDS)\n",
    "\n",
    "# Create a lookup table: obj_id -> index\n",
    "obj_id_to_idx = torch.full((max_obj_id + 1,), -1, dtype=torch.long, device=device)\n",
    "for idx, obj_id in enumerate(VALID_OBJ_IDS):\n",
    "    obj_id_to_idx[obj_id] = idx\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROTATION ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY ROTATION TRAINING SCRIPT\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP & HYPERPARAMETERS\n",
    "# ==========================================\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# --- PATHS ---\n",
    "# Define where your .ply models are located\n",
    "MODELS_DIR = '/kaggle/input/line-mode/Linemod_preprocessed/models' \n",
    "# List of valid object IDs in your dataset (must match your dataset logic)\n",
    "VALID_OBJ_IDS = [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15] \n",
    "\n",
    "# --- LOGGING SETUP ---\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# Directory to save checkpoints and logs\n",
    "CHECKPOINT_DIR = f'/kaggle/working/run_rotation' \n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "run_dir = CHECKPOINT_DIR\n",
    "\n",
    "print(f\"\\nðŸ”¥ STARTING ROTATION-ONLY TRAINING on {DEVICE}...\")\n",
    "print(f\"ðŸ“ Saving outputs to: {run_dir}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. INITIALIZE LOSS & MODELS\n",
    "# ==========================================\n",
    "\n",
    "# A. LOAD 3D POINTS FOR LOSS\n",
    "# We need to load the point clouds for all objects to use PointMatchingLoss.\n",
    "print(\"ðŸ“¦ Loading 3D Point Clouds for Loss Function...\")\n",
    "# Use the helper function we defined earlier to load all ply files\n",
    "point_bank = load_all_object_points(MODELS_DIR, VALID_OBJ_IDS, num_points=1000)\n",
    "point_bank = point_bank.to(DEVICE) # Move entire bank to GPU\n",
    "\n",
    "\n",
    "# B. DEFINE LOSS FUNCTION\n",
    "\n",
    "criterion = MultiObjectPointMatchingLoss(point_bank).to(DEVICE)\n",
    "\n",
    "# C. INITIALIZE MODEL\n",
    "# We only use the Rotation Network\n",
    "model_rot = ResNetRotation(freeze_backbone=False).to(DEVICE)\n",
    "\n",
    "# D. OPTIMIZER\n",
    "# We only optimize the rotation model parameters\n",
    "optimizer = optim.Adam(\n",
    "    model_rot.parameters(),\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# E. METRICS STORAGE\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# ==========================================\n",
    "# 3. TRAINING LOOP\n",
    "# ==========================================\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    # --- A. TRAIN PHASE ---\n",
    "    model_rot.train()\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    # Progress Bar\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        # 1. Move data to GPU\n",
    "        imgs = batch['image'].to(DEVICE)\n",
    "        gt_rot = batch['rotation'].to(DEVICE)\n",
    "        \n",
    "        # We need class indices for the PointMatchingLoss (Index 0 to 12)\n",
    "        # Ensure your Dataset returns 'class_id' as a mapped index (0..N), NOT the raw Linemod ID (1,5,8..)\n",
    "        obj_ids = batch['object_id'].to(DEVICE)\n",
    "        class_ids = obj_id_to_idx[obj_ids]\n",
    "        \n",
    "        # 2. Forward Pass\n",
    "        pred_rot = model_rot(imgs)\n",
    "\n",
    "        # 3. Calculate Loss\n",
    "        # Pass class_ids so the loss knows which 3D model to use for each image in the batch\n",
    "        if point_bank is not None:\n",
    "            loss = criterion(pred_rot, gt_rot, class_ids)\n",
    "        else:\n",
    "            loss = criterion(pred_rot, gt_rot) # Fallback doesn't use IDs\n",
    "\n",
    "        # 4. Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 5. Logging\n",
    "        running_train_loss += loss.item()\n",
    "        pbar.set_postfix({'ADD Loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # --- B. EVALUATION PHASE ---\n",
    "    model_rot.eval()\n",
    "    running_val_loss = 0.0\n",
    "    val_batches_limit = 50  # Validate on a subset to save time per epoch\n",
    "    count_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_iterator = islice(test_loader, val_batches_limit)\n",
    "        val_pbar = tqdm(val_iterator, total=val_batches_limit, desc=\"Validating\")\n",
    "\n",
    "        for batch in val_pbar:\n",
    "            imgs = batch['image'].to(DEVICE)\n",
    "            gt_rot = batch['rotation'].to(DEVICE)\n",
    "            # raw_names_list = batch['class_idx'] # es. ['can', 'ape', 'driller']\n",
    "            obj_ids = batch['object_id'].to(DEVICE)\n",
    "            class_ids = obj_id_to_idx[obj_ids]\n",
    "            \n",
    "\n",
    "            # Forward\n",
    "            pred_rot = model_rot(imgs)\n",
    "\n",
    "            # Loss\n",
    "            if point_bank is not None:\n",
    "                loss = criterion(pred_rot, gt_rot, class_ids)\n",
    "            else:\n",
    "                loss = criterion(pred_rot, gt_rot)\n",
    "                \n",
    "            running_val_loss += loss.item()\n",
    "            count_batches += 1\n",
    "\n",
    "    avg_val_loss = running_val_loss / count_batches if count_batches > 0 else 0\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # --- C. REPORT & SAVE ---\n",
    "    print(f\"ðŸ“Š Epoch {epoch+1} Summary: Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save Best Model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        save_path = os.path.join(CHECKPOINT_DIR, \"best_model_rot.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_rot.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss\n",
    "        }, save_path)\n",
    "        print(f\"ðŸ† New Best Rotation Model Saved! (Loss: {best_val_loss:.4f})\")\n",
    "\n",
    "    # Save Last Checkpoint (for resuming if needed)\n",
    "    if (epoch + 1) == NUM_EPOCHS:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_rot.state_dict(),\n",
    "            'val_loss': avg_val_loss\n",
    "        }, os.path.join(CHECKPOINT_DIR, f\"checkpoint_last.pth\"))\n",
    "\n",
    "# --- D. PLOTTING ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss (ADD Metric)')\n",
    "plt.plot(val_losses, label='Val Loss (ADD Metric)')\n",
    "plt.title('Rotation Training Convergence')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Distance (m)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, 'rotation_training_curve.png'))\n",
    "print(\"ðŸŽ‰ TRAINING COMPLETE! Training curve saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T10:28:56.836509Z",
     "iopub.status.busy": "2025-12-22T10:28:56.836192Z",
     "iopub.status.idle": "2025-12-22T10:37:25.797831Z",
     "shell.execute_reply": "2025-12-22T10:37:25.773835Z",
     "shell.execute_reply.started": "2025-12-22T10:28:56.836478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ONLY TRANSLATION TRAINING SCRIPT\n",
    "from src.pose_rgb import model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64        # Adjust if you run out of memory\n",
    "LR = 0.001             # Constant Learning Rate\n",
    "NUM_EPOCHS = 30\n",
    "CHECKPOINT_DIR = f'/kaggle/working/run_translation' \n",
    "DATA_ROOT = '/kaggle/input/line-mode/Linemod_preprocessed'\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL & OPTIMIZER SETUP\n",
    "# ==========================================\n",
    "print(f\"ðŸ§  Initializing Model on {DEVICE}...\")\n",
    "\n",
    "# Initialize your custom TranslationNet\n",
    "model_transl = TranslationNet()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"ðŸ”¥ Using {torch.cuda.device_count()} GPU!\")\n",
    "    model_transl = nn.DataParallel(model_transl)\n",
    "model_transl = model_transl.to(DEVICE)\n",
    "\n",
    "# Define Loss (Weighted to prioritize Depth Z)\n",
    "criterion = TranslationLoss(z_weight=1) \n",
    "\n",
    "# Scheduler and optimizer\n",
    "LR_MAX = 1e-3\n",
    "WEIGHT_DECAY = 1e-2\n",
    "EPOCHS = 30\n",
    "STEPS_PER_EPOCH = len(train_loader)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model_transl.parameters(), \n",
    "    lr=LR_MAX / 10,  # OneCycleLR handles the lr\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LR_MAX,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    pct_start=0.3,  # Il 30% del tempo speso a far salire il LR\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 3. TRAINING LOOP\n",
    "# ==========================================\n",
    "best_val_mae = float('inf') # Track the best error to save the best model\n",
    "\n",
    "print(\"ðŸš€ Starting Training Loop...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # --- TRAIN PHASE ---\n",
    "    model_transl.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Progress bar for training\n",
    "    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n",
    "    \n",
    "    for batch in train_loop:\n",
    "        # Move data to GPU\n",
    "        imgs = batch['image'].to(DEVICE)           # (B, 3, 224, 224)\n",
    "        bbox_info = batch['bbox_info'].to(DEVICE)  # (B, 4) Normalized BBox GPS\n",
    "        gt_trans = batch['translation'].to(DEVICE) # (B, 3) Absolute Translation in METERS\n",
    "\n",
    "        # Forward Pass\n",
    "        preds = model_transl(imgs, bbox_info)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = criterion(preds, gt_trans)\n",
    "        \n",
    "        # Backward Pass (Update Weights)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update stats\n",
    "        running_loss += loss.item()\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # --- VALIDATION PHASE ---\n",
    "    model_transl.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    # Variables to calculate error in Centimeters (for human readability)\n",
    "    error_sum_xyz = np.array([0.0, 0.0, 0.0]) \n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Eval]\"):\n",
    "            imgs = batch['image'].to(DEVICE)\n",
    "            bbox_info = batch['bbox_info'].to(DEVICE)\n",
    "            gt_trans = batch['translation'].to(DEVICE)\n",
    "\n",
    "            # Predict\n",
    "            preds = model_transl(imgs, bbox_info)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = criterion(preds, gt_trans)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate Absolute Error (in Meters)\n",
    "            abs_err = torch.abs(preds - gt_trans).cpu().numpy()\n",
    "            error_sum_xyz += abs_err.sum(axis=0)\n",
    "            total_samples += imgs.shape[0]\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    \n",
    "    # Convert Mean Error to Centimeters\n",
    "    mean_error_m = error_sum_xyz / total_samples\n",
    "    mean_error_cm = mean_error_m * 100.0\n",
    "    total_mae_cm = np.mean(mean_error_cm) # Average error across X, Y, Z\n",
    "\n",
    "    # --- REPORTING ---\n",
    "    print(f\"\\nðŸ“Š REPORT EPOCH {epoch+1}\")\n",
    "    print(f\"   Train Loss:    {avg_train_loss:.5f}\")\n",
    "    print(f\"   Val Loss:      {avg_val_loss:.5f}\")\n",
    "    print(f\"   --------------------------------\")\n",
    "    print(f\"   Error X:       {mean_error_cm[0]:.2f} cm\")\n",
    "    print(f\"   Error Y:       {mean_error_cm[1]:.2f} cm\")\n",
    "    print(f\"   Error Z:       {mean_error_cm[2]:.2f} cm (Depth)\")\n",
    "    print(f\"   --------------------------------\")\n",
    "    \n",
    "    # Save Best Model (if error is lower than previous best)\n",
    "    if total_mae_cm < best_val_mae:\n",
    "        best_val_mae = total_mae_cm\n",
    "        torch.save(model_transl.state_dict(), f\"{CHECKPOINT_DIR}/best_translation_model.pth\")\n",
    "        print(f\"   ðŸ’¾ New Best Model Saved! (Avg Error: {total_mae_cm:.2f} cm)\")\n",
    "        \n",
    "    # Save periodic checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model_transl.state_dict(), f\"{CHECKPOINT_DIR}/translation_ep{epoch+1}.pth\")\n",
    "\n",
    "print(\"\\nâœ… Training Complete. Best model saved in:\", CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fcaro\\.virtualenvs\\mlVenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Initializing Model on cpu...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m WEIGHT_DECAY = \u001b[32m1e-2\u001b[39m\n\u001b[32m     41\u001b[39m EPOCHS = \u001b[32m30\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m STEPS_PER_EPOCH = \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_loader\u001b[49m)\n\u001b[32m     44\u001b[39m optimizer = optim.AdamW(\n\u001b[32m     45\u001b[39m     model_transl.parameters(), \n\u001b[32m     46\u001b[39m     lr=LR_MAX / \u001b[32m10\u001b[39m,  \u001b[38;5;66;03m# OneCycleLR handles the lr\u001b[39;00m\n\u001b[32m     47\u001b[39m     weight_decay=WEIGHT_DECAY\n\u001b[32m     48\u001b[39m )\n\u001b[32m     50\u001b[39m scheduler = optim.lr_scheduler.OneCycleLR(\n\u001b[32m     51\u001b[39m     optimizer,\n\u001b[32m     52\u001b[39m     max_lr=LR_MAX,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     anneal_strategy=\u001b[33m'\u001b[39m\u001b[33mcos\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     57\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# ONLY TRANSLATION TRAINING SCRIPT\n",
    "from src.pose_rgb.model import UnifiedPoseNet\n",
    "from src.pose_rgb.loss import UnifiedPoseLoss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64        # Adjust if you run out of memory\n",
    "LR = 0.001             # Constant Learning Rate\n",
    "NUM_EPOCHS = 30\n",
    "CHECKPOINT_DIR = f'/kaggle/working/run_translation' \n",
    "DATA_ROOT = '/kaggle/input/line-mode/Linemod_preprocessed'\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL & OPTIMIZER SETUP\n",
    "# ==========================================\n",
    "print(f\"ðŸ§  Initializing Model on {DEVICE}...\")\n",
    "\n",
    "# Initialize your custom TranslationNet\n",
    "model = UnifiedPoseNet(freeze_backbone=False)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"ðŸ”¥ Using {torch.cuda.device_count()} GPU!\")\n",
    "    model_transl = nn.DataParallel(model_transl)\n",
    "\n",
    "\n",
    "criterion = UnifiedPoseLoss(w_rot=1.0, w_trans=2.0).to(DEVICE)\n",
    "\n",
    "# Scheduler and optimizer\n",
    "LR_MAX = 1e-3\n",
    "WEIGHT_DECAY = 1e-2\n",
    "EPOCHS = 30\n",
    "STEPS_PER_EPOCH = len(train_loader)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model_transl.parameters(), \n",
    "    lr=LR_MAX / 10,  # OneCycleLR handles the lr\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LR_MAX,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    pct_start=0.3,  # 30% of the time increasing LR\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 3. TRAINING LOOP\n",
    "# ==========================================\n",
    "best_val_mae = float('inf') # Track the best error to save the best model\n",
    "\n",
    "print(\"ðŸš€ Starting Training Loop...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # --- TRAIN PHASE ---\n",
    "    model_transl.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Progress bar for training\n",
    "    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n",
    "    \n",
    "    for batch in train_loop:\n",
    "        # Move data to GPU\n",
    "        imgs = batch['image'].to(DEVICE)           # (B, 3, 224, 224)\n",
    "        bbox_info = batch['bbox_info'].to(DEVICE)  # (B, 4) Normalized BBox GPS\n",
    "        gt_trans = batch['translation'].to(DEVICE) # (B, 3) Absolute Translation in METERS\n",
    "        gt_rot = batch['rotation'].to(DEVICE)\n",
    "\n",
    "        # Forward Pass\n",
    "        pred_rot, pred_trans = model(imgs, bbox_info)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = criterion(pred_rot, pred_trans, gt_rot, gt_trans)\n",
    "        \n",
    "        # Backward Pass (Update Weights)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update stats\n",
    "        running_loss += loss.item()\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # --- VALIDATION PHASE ---\n",
    "    model_transl.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    # Variables to calculate error in Centimeters (for human readability)\n",
    "    error_sum_xyz = np.array([0.0, 0.0, 0.0]) \n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Eval]\"):\n",
    "            imgs = batch['image'].to(DEVICE)\n",
    "            bbox_info = batch['bbox_info'].to(DEVICE)\n",
    "            gt_trans = batch['translation'].to(DEVICE)\n",
    "\n",
    "            # Predict\n",
    "            preds = model_transl(imgs, bbox_info)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = criterion(preds, gt_trans)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate Absolute Error (in Meters)\n",
    "            abs_err = torch.abs(preds - gt_trans).cpu().numpy()\n",
    "            error_sum_xyz += abs_err.sum(axis=0)\n",
    "            total_samples += imgs.shape[0]\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    \n",
    "    # Convert Mean Error to Centimeters\n",
    "    mean_error_m = error_sum_xyz / total_samples\n",
    "    mean_error_cm = mean_error_m * 100.0\n",
    "    total_mae_cm = np.mean(mean_error_cm) # Average error across X, Y, Z\n",
    "\n",
    "    # --- REPORTING ---\n",
    "    print(f\"\\nðŸ“Š REPORT EPOCH {epoch+1}\")\n",
    "    print(f\"   Train Loss:    {avg_train_loss:.5f}\")\n",
    "    print(f\"   Val Loss:      {avg_val_loss:.5f}\")\n",
    "    print(f\"   --------------------------------\")\n",
    "    print(f\"   Error X:       {mean_error_cm[0]:.2f} cm\")\n",
    "    print(f\"   Error Y:       {mean_error_cm[1]:.2f} cm\")\n",
    "    print(f\"   Error Z:       {mean_error_cm[2]:.2f} cm (Depth)\")\n",
    "    print(f\"   --------------------------------\")\n",
    "    \n",
    "    # Save Best Model (if error is lower than previous best)\n",
    "    if total_mae_cm < best_val_mae:\n",
    "        best_val_mae = total_mae_cm\n",
    "        torch.save(model_transl.state_dict(), f\"{CHECKPOINT_DIR}/best_translation_model.pth\")\n",
    "        print(f\"   ðŸ’¾ New Best Model Saved! (Avg Error: {total_mae_cm:.2f} cm)\")\n",
    "        \n",
    "    # Save periodic checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model_transl.state_dict(), f\"{CHECKPOINT_DIR}/translation_ep{epoch+1}.pth\")\n",
    "\n",
    "print(\"\\nâœ… Training Complete. Best model saved in:\", CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading 3D model points and diameters...\n",
      "ðŸ“¦ Loading trained model...\n",
      "ðŸ“š Preparing test dataset and dataloader...\n",
      " Loaded LineModPoseDepthDataset\n",
      "   Split: test (Ratio: 0.20)\n",
      "   Objects: [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "   Total samples: 4867\n",
      "\n",
      "ðŸš€ Starting Comprehensive Benchmark (ADD Error + ADD-0.1d Accuracy)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [16:03<00:00, 24.70s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Evaluation report saved to ./RGB_run/evaluation_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Object ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Object Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Diameter (mm)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Mean ADD (mm)",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "Mean ADD-S (mm)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Mean ADD-Rot (mm)",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "Mean ADD-S-Rot (mm)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ADD-0.1d Accuracy (%)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ADD-S-0.1d Accuracy (%)",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "65e66c02-a04f-4a2c-bf74-f4c2ba6a16f3",
       "rows": [
        [
         "0",
         "1",
         "ape",
         "102.09865663",
         "6.3711176",
         "2.1292544287237",
         "6.3711176",
         "2.1292544287237",
         "86.86440677966102",
         "99.78813559322035"
        ],
        [
         "1",
         "2",
         "benchvise",
         "247.50624233",
         "12.7289715",
         "5.129824546231763",
         "12.7289715",
         "5.129824546231763",
         "92.14876033057851",
         "100.0"
        ],
        [
         "2",
         "5",
         "can",
         "201.40358597",
         "10.640517",
         "3.6934506261609434",
         "10.640517",
         "3.6934506261609434",
         "93.9203354297694",
         "99.79035639412997"
        ],
        [
         "3",
         "6",
         "cat",
         "154.54551808",
         "7.6868267",
         "3.226671479561668",
         "7.6868267",
         "3.226671479561668",
         "92.37472766884531",
         "99.12854030501089"
        ],
        [
         "4",
         "8",
         "driller",
         "261.47178102",
         "11.04353",
         "5.103863760619234",
         "11.04353",
         "5.103863760619234",
         "96.21052631578947",
         "100.0"
        ],
        [
         "5",
         "9",
         "duck",
         "108.99920102",
         "8.140133",
         "2.7549539564965917",
         "8.140133",
         "2.7549539564965917",
         "83.60995850622407",
         "99.79253112033194"
        ],
        [
         "6",
         "10",
         "eggbox",
         "164.62758848",
         "10.934605",
         "3.253343872492656",
         "10.934605",
         "3.253343872492656",
         "87.73148148148148",
         "100.0"
        ],
        [
         "7",
         "11",
         "glue",
         "175.88933422",
         "8.390186",
         "3.7778694538121496",
         "8.390186",
         "3.7778694538121496",
         "95.16129032258065",
         "100.0"
        ],
        [
         "8",
         "12",
         "holepuncher",
         "145.54287471",
         "9.141986",
         "2.895302563732996",
         "9.141986",
         "2.895302563732996",
         "85.91836734693878",
         "100.0"
        ],
        [
         "9",
         "4",
         "camera",
         "172.49224865",
         "8.40986",
         "3.107157819201605",
         "8.40986",
         "3.107157819201605",
         "95.0207468879668",
         "100.0"
        ],
        [
         "10",
         "13",
         "iron",
         "278.07811733",
         "12.014362",
         "5.025002339360982",
         "12.014362",
         "5.025002339360982",
         "95.23809523809523",
         "100.0"
        ],
        [
         "11",
         "14",
         "lamp",
         "282.60129399",
         "11.619051",
         "4.726791443154207",
         "11.619051",
         "4.726791443154207",
         "98.36734693877551",
         "100.0"
        ],
        [
         "12",
         "15",
         "phone",
         "212.35825148",
         "12.144069",
         "4.6267420177680245",
         "12.144069",
         "4.6267420177680245",
         "90.76305220883533",
         "100.0"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 13
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Object ID</th>\n",
       "      <th>Object Name</th>\n",
       "      <th>Diameter (mm)</th>\n",
       "      <th>Mean ADD (mm)</th>\n",
       "      <th>Mean ADD-S (mm)</th>\n",
       "      <th>Mean ADD-Rot (mm)</th>\n",
       "      <th>Mean ADD-S-Rot (mm)</th>\n",
       "      <th>ADD-0.1d Accuracy (%)</th>\n",
       "      <th>ADD-S-0.1d Accuracy (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ape</td>\n",
       "      <td>102.098657</td>\n",
       "      <td>6.371118</td>\n",
       "      <td>2.129254</td>\n",
       "      <td>6.371118</td>\n",
       "      <td>2.129254</td>\n",
       "      <td>86.864407</td>\n",
       "      <td>99.788136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>benchvise</td>\n",
       "      <td>247.506242</td>\n",
       "      <td>12.728971</td>\n",
       "      <td>5.129825</td>\n",
       "      <td>12.728971</td>\n",
       "      <td>5.129825</td>\n",
       "      <td>92.148760</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>can</td>\n",
       "      <td>201.403586</td>\n",
       "      <td>10.640517</td>\n",
       "      <td>3.693451</td>\n",
       "      <td>10.640517</td>\n",
       "      <td>3.693451</td>\n",
       "      <td>93.920335</td>\n",
       "      <td>99.790356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>cat</td>\n",
       "      <td>154.545518</td>\n",
       "      <td>7.686827</td>\n",
       "      <td>3.226671</td>\n",
       "      <td>7.686827</td>\n",
       "      <td>3.226671</td>\n",
       "      <td>92.374728</td>\n",
       "      <td>99.128540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>driller</td>\n",
       "      <td>261.471781</td>\n",
       "      <td>11.043530</td>\n",
       "      <td>5.103864</td>\n",
       "      <td>11.043530</td>\n",
       "      <td>5.103864</td>\n",
       "      <td>96.210526</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>duck</td>\n",
       "      <td>108.999201</td>\n",
       "      <td>8.140133</td>\n",
       "      <td>2.754954</td>\n",
       "      <td>8.140133</td>\n",
       "      <td>2.754954</td>\n",
       "      <td>83.609959</td>\n",
       "      <td>99.792531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>eggbox</td>\n",
       "      <td>164.627588</td>\n",
       "      <td>10.934605</td>\n",
       "      <td>3.253344</td>\n",
       "      <td>10.934605</td>\n",
       "      <td>3.253344</td>\n",
       "      <td>87.731481</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>glue</td>\n",
       "      <td>175.889334</td>\n",
       "      <td>8.390186</td>\n",
       "      <td>3.777869</td>\n",
       "      <td>8.390186</td>\n",
       "      <td>3.777869</td>\n",
       "      <td>95.161290</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>holepuncher</td>\n",
       "      <td>145.542875</td>\n",
       "      <td>9.141986</td>\n",
       "      <td>2.895303</td>\n",
       "      <td>9.141986</td>\n",
       "      <td>2.895303</td>\n",
       "      <td>85.918367</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>camera</td>\n",
       "      <td>172.492249</td>\n",
       "      <td>8.409860</td>\n",
       "      <td>3.107158</td>\n",
       "      <td>8.409860</td>\n",
       "      <td>3.107158</td>\n",
       "      <td>95.020747</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>iron</td>\n",
       "      <td>278.078117</td>\n",
       "      <td>12.014362</td>\n",
       "      <td>5.025002</td>\n",
       "      <td>12.014362</td>\n",
       "      <td>5.025002</td>\n",
       "      <td>95.238095</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>lamp</td>\n",
       "      <td>282.601294</td>\n",
       "      <td>11.619051</td>\n",
       "      <td>4.726791</td>\n",
       "      <td>11.619051</td>\n",
       "      <td>4.726791</td>\n",
       "      <td>98.367347</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>phone</td>\n",
       "      <td>212.358251</td>\n",
       "      <td>12.144069</td>\n",
       "      <td>4.626742</td>\n",
       "      <td>12.144069</td>\n",
       "      <td>4.626742</td>\n",
       "      <td>90.763052</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Object ID  Object Name  Diameter (mm)  Mean ADD (mm)  Mean ADD-S (mm)  \\\n",
       "0           1          ape     102.098657       6.371118         2.129254   \n",
       "1           2    benchvise     247.506242      12.728971         5.129825   \n",
       "2           5          can     201.403586      10.640517         3.693451   \n",
       "3           6          cat     154.545518       7.686827         3.226671   \n",
       "4           8      driller     261.471781      11.043530         5.103864   \n",
       "5           9         duck     108.999201       8.140133         2.754954   \n",
       "6          10       eggbox     164.627588      10.934605         3.253344   \n",
       "7          11         glue     175.889334       8.390186         3.777869   \n",
       "8          12  holepuncher     145.542875       9.141986         2.895303   \n",
       "9           4       camera     172.492249       8.409860         3.107158   \n",
       "10         13         iron     278.078117      12.014362         5.025002   \n",
       "11         14         lamp     282.601294      11.619051         4.726791   \n",
       "12         15        phone     212.358251      12.144069         4.626742   \n",
       "\n",
       "    Mean ADD-Rot (mm)  Mean ADD-S-Rot (mm)  ADD-0.1d Accuracy (%)  \\\n",
       "0            6.371118             2.129254              86.864407   \n",
       "1           12.728971             5.129825              92.148760   \n",
       "2           10.640517             3.693451              93.920335   \n",
       "3            7.686827             3.226671              92.374728   \n",
       "4           11.043530             5.103864              96.210526   \n",
       "5            8.140133             2.754954              83.609959   \n",
       "6           10.934605             3.253344              87.731481   \n",
       "7            8.390186             3.777869              95.161290   \n",
       "8            9.141986             2.895303              85.918367   \n",
       "9            8.409860             3.107158              95.020747   \n",
       "10          12.014362             5.025002              95.238095   \n",
       "11          11.619051             4.726791              98.367347   \n",
       "12          12.144069             4.626742              90.763052   \n",
       "\n",
       "    ADD-S-0.1d Accuracy (%)  \n",
       "0                 99.788136  \n",
       "1                100.000000  \n",
       "2                 99.790356  \n",
       "3                 99.128540  \n",
       "4                100.000000  \n",
       "5                 99.792531  \n",
       "6                100.000000  \n",
       "7                100.000000  \n",
       "8                100.000000  \n",
       "9                100.000000  \n",
       "10               100.000000  \n",
       "11               100.000000  \n",
       "12               100.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.pose_rgb.evaluate import evaluate_RGB, evaluate_RGB_rot_only\n",
    "\n",
    "model_path = \"./RGB_run/best_model_rot.pth\"\n",
    "dataset_root = \"../../Linemod_preprocessed\"\n",
    "output_path = \"./RGB_run/evaluation_results.csv\"\n",
    "\n",
    "\n",
    "df = evaluate_RGB_rot_only(\n",
    "    model_path=model_path,\n",
    "    dataset_root=dataset_root,\n",
    "    output_path=output_path\n",
    ")\n",
    "    \n",
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8862865,
     "sourceId": 13909860,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mlVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
