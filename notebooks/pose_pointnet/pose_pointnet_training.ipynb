{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13909860,"sourceType":"datasetVersion","datasetId":8862865}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"25573688","cell_type":"code","source":"import os\nimport sys\n\n# Clone or pull part\nrepo_url = \"https://github.com/fraco03/6D_pose.git\"\nrepo_dir = \"/kaggle/working/6D_pose\"   #Modify here for kaggle\nbranch = \"pose_Pointnet\"\n\n# Clone if missing\nif not os.path.exists(repo_dir):\n    !git clone -b {branch} {repo_url}\n    print(f\"Cloned {repo_url} to {repo_dir}\")\nelse:\n    %cd {repo_dir}\n    !git fetch origin\n    !git checkout {branch}\n    !git reset --hard origin/{branch}\n    # %cd ..\n    print(f\"Updated {repo_url} to {repo_dir}\")\n\n# Add repository to Python path\nif repo_dir not in sys.path:\n    sys.path.insert(0, repo_dir)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-20T20:38:49.743572Z","iopub.execute_input":"2025-12-20T20:38:49.743745Z","iopub.status.idle":"2025-12-20T20:38:51.063858Z","shell.execute_reply.started":"2025-12-20T20:38:49.743726Z","shell.execute_reply":"2025-12-20T20:38:51.063019Z"},"id":"25573688","outputId":"72321497-6c3a-436b-90f0-fd5df011e398","trusted":true},"outputs":[{"name":"stdout","text":"Cloning into '6D_pose'...\nremote: Enumerating objects: 659, done.\u001b[K\nremote: Counting objects: 100% (325/325), done.\u001b[K\nremote: Compressing objects: 100% (229/229), done.\u001b[K\nremote: Total 659 (delta 183), reused 202 (delta 96), pack-reused 334 (from 1)\u001b[K\nReceiving objects: 100% (659/659), 9.44 MiB | 29.02 MiB/s, done.\nResolving deltas: 100% (349/349), done.\nCloned https://github.com/fraco03/6D_pose.git to /kaggle/working/6D_pose\n","output_type":"stream"}],"execution_count":1},{"id":"607d2f50-66f8-4df1-883c-3435ae98f202","cell_type":"code","source":"sys.path.insert(0,\"/kaggle/working/6D_pose\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T20:38:52.924037Z","iopub.execute_input":"2025-12-20T20:38:52.924396Z","iopub.status.idle":"2025-12-20T20:38:52.929663Z","shell.execute_reply.started":"2025-12-20T20:38:52.924361Z","shell.execute_reply":"2025-12-20T20:38:52.928738Z"}},"outputs":[],"execution_count":2},{"id":"MdEhRyP0oHcN","cell_type":"code","source":"%cd ..\n!gdown --fuzzy https://drive.google.com/file/d/1zNthSyiBdPUfn7BmUKPbKoGgQdG1vGnS/view?usp=drive_link -O Linemod_preprocessed.zip\n!unzip Linemod_preprocessed.zip\n%cd 6D_pose","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"execution_failed":"2025-12-20T11:15:16.832Z"},"id":"MdEhRyP0oHcN","outputId":"777a0c2c-83d7-4db1-b21c-05a812fa8bc0","trusted":true},"outputs":[],"execution_count":null},{"id":"35e78866","cell_type":"code","source":"from google.colab import drive\nfrom utils.load_data import mount_drive\n\n# Mounting part\nmount_drive()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35e78866","outputId":"b5cd53ec-326c-40a1-a4d8-f7fec238b55d"},"outputs":[],"execution_count":null},{"id":"2c40e2b0-ebc6-412b-b2db-6f36c33f0b5e","cell_type":"code","source":"%mv Linemod_preprocessed working/","metadata":{"execution":{"iopub.execute_input":"2025-12-20T09:57:27.976090Z","iopub.status.busy":"2025-12-20T09:57:27.975476Z","iopub.status.idle":"2025-12-20T09:57:46.017384Z","shell.execute_reply":"2025-12-20T09:57:46.016449Z","shell.execute_reply.started":"2025-12-20T09:57:27.976041Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"d6bbd0dc","cell_type":"code","source":"# dataset_root = \"/content/drive/MyDrive/Linemod_preprocessed\" #Modify here for kaggle\n# dataset_root = \"../../Linemod_preprocessed_small\"\n# dataset_root = \"/content/Linemod_preprocessed\"\n# dataset_root = \"/kaggle/working/Linemod_preprocessed\"\ndataset_root = \"/kaggle/input/line-mode/Linemod_preprocessed\"\n\nprint(\"\\n‚úÖ Setup complete!\")\nprint(f\"üìÅ Dataset path: {dataset_root}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-20T20:38:56.743168Z","iopub.execute_input":"2025-12-20T20:38:56.743918Z","iopub.status.idle":"2025-12-20T20:38:56.748334Z","shell.execute_reply.started":"2025-12-20T20:38:56.743887Z","shell.execute_reply":"2025-12-20T20:38:56.747460Z"},"id":"d6bbd0dc","outputId":"a3326ecf-a530-4f9e-d68f-a8ba83bcabb8","trusted":true},"outputs":[{"name":"stdout","text":"\n‚úÖ Setup complete!\nüìÅ Dataset path: /kaggle/input/line-mode/Linemod_preprocessed\n","output_type":"stream"}],"execution_count":3},{"id":"4728378e","cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"id":"4728378e"},"outputs":[],"execution_count":null},{"id":"598da66d-fc4d-4d42-b8d8-52104f3947c9","cell_type":"code","source":"%mv Linemod_preprocessed ./working","metadata":{"execution":{"iopub.execute_input":"2025-12-19T16:29:25.943772Z","iopub.status.busy":"2025-12-19T16:29:25.943473Z","iopub.status.idle":"2025-12-19T16:29:43.917596Z","shell.execute_reply":"2025-12-19T16:29:43.916652Z","shell.execute_reply.started":"2025-12-19T16:29:25.943738Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"b4gbKJycp4vr","cell_type":"code","source":"!pip install plyfile","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-20T20:38:59.090554Z","iopub.execute_input":"2025-12-20T20:38:59.091298Z","iopub.status.idle":"2025-12-20T20:39:03.130796Z","shell.execute_reply.started":"2025-12-20T20:38:59.091266Z","shell.execute_reply":"2025-12-20T20:39:03.129871Z"},"id":"b4gbKJycp4vr","outputId":"1a37cca0-76ab-4675-df0b-becd827133ce","trusted":true},"outputs":[{"name":"stdout","text":"Collecting plyfile\n  Downloading plyfile-1.1.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from plyfile) (2.0.2)\nDownloading plyfile-1.1.3-py3-none-any.whl (36 kB)\nInstalling collected packages: plyfile\nSuccessfully installed plyfile-1.1.3\n","output_type":"stream"}],"execution_count":4},{"id":"a443ec54","cell_type":"code","source":"import sys\n\nsys.path.append('../..')","metadata":{},"outputs":[],"execution_count":null},{"id":"75cc69f7","cell_type":"code","source":"from src.pose_pointnet.dataset import PointNetLineModDataset\n\ntrain_dataset = PointNetLineModDataset(\n    root_dir=dataset_root,\n    split=\"train\"\n)\n\ntest_dataset = PointNetLineModDataset(\n    root_dir=dataset_root,\n    split=\"test\"\n)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-20T20:39:03.132337Z","iopub.execute_input":"2025-12-20T20:39:03.132601Z","iopub.status.idle":"2025-12-20T20:40:20.114937Z","shell.execute_reply.started":"2025-12-20T20:39:03.132576Z","shell.execute_reply":"2025-12-20T20:40:20.114280Z"},"id":"75cc69f7","outputId":"4a87d952-f2cf-4e57-d26c-eeabf1bdc09d","trusted":true},"outputs":[{"name":"stdout","text":"‚úÖ LineModConfig initialized: /kaggle/input/line-mode/Linemod_preprocessed\n‚úÖ Loaded PointNetLineModDataset\n   Split: train (Ratio: 0.8)\n   Num Points: 1024\n   Total samples: 12634\n‚úÖ Loaded PointNetLineModDataset\n   Split: test (Ratio: 0.8)\n   Num Points: 1024\n   Total samples: 3166\n","output_type":"stream"}],"execution_count":5},{"id":"0aea679a","cell_type":"code","source":"import torch\nsample = train_dataset[0]\n\nprint(f\"Sample keys: {sample.keys()}\")\nfor key, value in sample.items():\n    if isinstance(value, torch.Tensor):\n        print(f\"  {key}: Tensor of shape {value.shape} and dtype {value.dtype}\")\n    else:\n        print(f\"  {key}: {type(value)} with value {value}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-20T20:40:22.506670Z","iopub.execute_input":"2025-12-20T20:40:22.507510Z","iopub.status.idle":"2025-12-20T20:40:22.544070Z","shell.execute_reply.started":"2025-12-20T20:40:22.507480Z","shell.execute_reply":"2025-12-20T20:40:22.543502Z"},"id":"0aea679a","outputId":"9280a363-50e8-45a0-c1d1-32a7f0ff340d","trusted":true},"outputs":[{"name":"stdout","text":"Sample keys: dict_keys(['points', 'centroid', 'rotation', 't_residual', 'gt_translation', 'object_id', 'class_idx', 'img_id', 'cam_K'])\n  points: Tensor of shape torch.Size([3, 1024]) and dtype torch.float32\n  centroid: Tensor of shape torch.Size([3]) and dtype torch.float32\n  rotation: Tensor of shape torch.Size([4]) and dtype torch.float32\n  t_residual: Tensor of shape torch.Size([3]) and dtype torch.float32\n  gt_translation: Tensor of shape torch.Size([3]) and dtype torch.float32\n  object_id: <class 'int'> with value 1\n  class_idx: <class 'str'> with value ape\n  img_id: <class 'int'> with value 987\n  cam_K: Tensor of shape torch.Size([3, 3]) and dtype torch.float32\n","output_type":"stream"}],"execution_count":6},{"id":"82a727c4","cell_type":"code","source":"import torch\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-20T20:41:48.233896Z","iopub.execute_input":"2025-12-20T20:41:48.234676Z","iopub.status.idle":"2025-12-20T20:41:48.318379Z","shell.execute_reply.started":"2025-12-20T20:41:48.234645Z","shell.execute_reply":"2025-12-20T20:41:48.317619Z"},"id":"82a727c4","outputId":"2eb9ff33-3511-4a32-8d88-73e9c3dec89a","trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":8},{"id":"12ce6568","cell_type":"code","source":"from utils.linemod_config import get_linemod_config\nimport numpy as np\nimport torch\n\n\nlinemod_config = get_linemod_config(dataset_root)\n\nall_model_points = []\nNUM_POINTS = 1000  # Number of points to sample from each model\nVALID_OBJ_IDS = [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15] \nfor obj_id in VALID_OBJ_IDS:\n    model_points = linemod_config.get_model_3d(obj_id, unit='m')  # (N, 3)\n    if model_points.shape[0] >= NUM_POINTS:\n        choice = np.random.choice(model_points.shape[0], NUM_POINTS, replace=False)\n    else:\n        choice = np.random.choice(model_points.shape[0], NUM_POINTS, replace=True)\n    model_points = model_points[choice, :]\n    all_model_points.append(torch.tensor(model_points, dtype=torch.float32))\nall_model_points = torch.stack(all_model_points, dim=0)  # (Num_Classes, NUM_POINTS, 3)\nall_model_points = all_model_points.to(device)\n\nmax_obj_id = max(VALID_OBJ_IDS)\n\n# Create a lookup table: obj_id -> index\nobj_id_to_idx = torch.full((max_obj_id + 1,), -1, dtype=torch.long, device=device)\nfor idx, obj_id in enumerate(VALID_OBJ_IDS):\n    obj_id_to_idx[obj_id] = idx\n","metadata":{"execution":{"iopub.status.busy":"2025-12-20T20:41:52.485989Z","iopub.execute_input":"2025-12-20T20:41:52.486634Z","iopub.status.idle":"2025-12-20T20:41:52.499891Z","shell.execute_reply.started":"2025-12-20T20:41:52.486603Z","shell.execute_reply":"2025-12-20T20:41:52.499231Z"},"trusted":true},"outputs":[],"execution_count":10},{"id":"e87b1b58-4804-474a-8116-8c1f089220a0","cell_type":"code","source":"all_model_points.shape","metadata":{"execution":{"iopub.status.busy":"2025-12-20T20:41:56.190193Z","iopub.execute_input":"2025-12-20T20:41:56.190855Z","iopub.status.idle":"2025-12-20T20:41:56.196117Z","shell.execute_reply.started":"2025-12-20T20:41:56.190825Z","shell.execute_reply":"2025-12-20T20:41:56.195563Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"torch.Size([13, 1000, 3])"},"metadata":{}}],"execution_count":11},{"id":"f220786c","cell_type":"code","source":"from src.pose_pointnet.loss import MultiObjectPointMatchingLoss\nimport torch.nn as nn\nfrom src.pose_pointnet.model import PointNetPoseModel\nfrom torch.optim import Adam, AdamW\n\nmodel = PointNetPoseModel()\n\nif torch.cuda.device_count() > 1:\n    print(f\"üî• Using {torch.cuda.device_count()} GPU!\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(device)\n\n# Loss function and optimizer\ncriterion = MultiObjectPointMatchingLoss(all_model_points)\noptimizer = AdamW(\n    model.parameters(), \n    lr=0.001, \n    betas=(0.9, 0.999), \n    weight_decay=1e-4\n)\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, \n    T_max=100,      # Imposta il numero delle tue epoche totali qui!\n    eta_min=1e-5\n)","metadata":{"execution":{"iopub.status.busy":"2025-12-20T20:46:00.990883Z","iopub.execute_input":"2025-12-20T20:46:00.991546Z","iopub.status.idle":"2025-12-20T20:46:01.011030Z","shell.execute_reply.started":"2025-12-20T20:46:00.991507Z","shell.execute_reply":"2025-12-20T20:46:01.010269Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üî• Using 2 GPU!\n","output_type":"stream"}],"execution_count":19},{"id":"62482a0e-ba6c-4ec0-8682-5bfbbe6978dc","cell_type":"code","source":"def set_bn_momentum_default(curr_epoch, total_epochs, model):\n    \"\"\"\n    Decade il momentum della Batch Norm da 0.1 a 0.01 seguendo una curva a step o coseno.\n    In PyTorch il default momentum √® 0.1.\n    \"\"\"\n    # Formula originale PointNet: decade del 50% ogni tot step\n    # Qui usiamo una versione coseno pi√π moderna e fluida\n    \n    # Calcola il momentum target: parte da 0.1 e scende a 0.01\n    start_mom = 0.1\n    end_mom = 0.01\n    \n    # Interpolazione semplice basata sull'epoca\n    momentum = end_mom + (start_mom - end_mom) * (1 - (curr_epoch / total_epochs))\n    \n    # Applica a tutti i layer di Batch Norm nel modello\n    for m in model.modules():\n        if isinstance(m, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d)):\n            m.momentum = momentum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T20:44:08.348248Z","iopub.execute_input":"2025-12-20T20:44:08.349026Z","iopub.status.idle":"2025-12-20T20:44:08.353880Z","shell.execute_reply.started":"2025-12-20T20:44:08.348999Z","shell.execute_reply":"2025-12-20T20:44:08.353251Z"}},"outputs":[],"execution_count":13},{"id":"bbf4ae51-b4ce-4a9b-87d5-1fbe4b19e6bf","cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 128  #double GPU\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2025-12-20T20:44:12.634941Z","iopub.execute_input":"2025-12-20T20:44:12.635716Z","iopub.status.idle":"2025-12-20T20:44:12.640019Z","shell.execute_reply.started":"2025-12-20T20:44:12.635684Z","shell.execute_reply":"2025-12-20T20:44:12.639262Z"},"trusted":true},"outputs":[],"execution_count":14},{"id":"6666a692","cell_type":"code","source":"from tqdm import tqdm\nimport os\nimport torch\nfrom datetime import datetime\n\n# ==========================================\n# 0. SETUP AND CONFIGURATION\n# ==========================================\nnum_epochs = 100  # PointNet converges relatively fast\nbest_test_loss = float('inf')\nbatch_size = 32  # Adjust based on your GPU VRAM\n\n# Setup checkpoint directory\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\ncheckpoint_dir = f'/kaggle/working/POINTNET_{timestamp}'\n# checkpoint_dir = f'./POINTNET_{timestamp}'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Trackers for plotting\ntrain_losses = []\ntest_losses = []\n\nprint(f\"üöÄ Starting PointNet Training on {device}\")\nprint(f\"üìÅ Checkpoints will be saved to: {checkpoint_dir}\")\nprint(f\"üó∫Ô∏è  Object ID Mapping created for {len(obj_id_to_idx)} objects.\")\n\n# ==========================================\n# 1. TRAINING LOOP\n# ==========================================\nfor epoch in range(num_epochs):\n    set_bn_momentum_default(epoch, num_epochs, model)\n    \n    model.train()\n    epoch_loss = 0.0\n\n    # Initialize progress bar\n    train_pbar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Training\")\n    \n    for batch in train_pbar:\n        # Move data to GPU\n        # PointNet input: (Batch, 3, Num_Points)\n        points = batch['points'].to(device)  \n        \n        # Auxiliary data for reconstruction and loss\n        centroids = batch['centroid'].to(device)       # (B, 3)\n        gt_rotations = batch['rotation'].to(device)    # (B, 4)\n        gt_t_absolute = batch['gt_translation'].to(device) # (B, 3) - Absolute target\n        \n        # Handle Object IDs for Loss Indexing\n        raw_obj_ids = batch['object_id'].tolist()\n        # Map raw IDs (e.g., 15) to buffer indices (e.g., 12)\n        target_indices = torch.tensor(\n            [obj_id_to_idx[oid] for oid in raw_obj_ids], \n            dtype=torch.long, device=device\n        )\n\n        # --- FORWARD PASS ---\n        optimizer.zero_grad()\n        \n        # The network predicts: \n        # 1. Quaternion (pred_q)\n        # 2. Residual Translation relative to centroid (pred_t_res)\n        pred_q, pred_t_res = model(points)\n\n        # --- RECONSTRUCTION ---\n        # Reconstruct absolute translation for the ADD Loss\n        # Absolute_Pos = Centroid + Residual\n        pred_t_abs = centroids + pred_t_res\n\n        # --- LOSS CALCULATION ---\n        # Using MultiObjectPointMatchingLoss (ADD metric)\n        loss = criterion(\n            pred_q=pred_q, \n            pred_t=pred_t_abs,   # Pass the reconstructed absolute translation\n            gt_q=gt_rotations, \n            gt_t=gt_t_absolute, \n            class_indices=target_indices\n        )\n        \n        loss.backward()\n        optimizer.step()\n\n        # Update stats\n        epoch_loss += loss.item()\n        train_pbar.set_postfix({'ADD Loss (m)': f\"{loss.item():.4f}\"})\n\n    # Calculate average training loss\n    avg_train_loss = epoch_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    scheduler.step()\n\n    # ==========================================\n    # 2. VALIDATION LOOP\n    # ==========================================\n    model.eval()\n    test_loss = 0.0\n    val_pbar = tqdm(test_loader, desc=\"Validating\")\n    \n    with torch.no_grad():\n        for batch in val_pbar:\n            # Move data to GPU\n            points = batch['points'].to(device)\n            centroids = batch['centroid'].to(device)\n            gt_rotations = batch['rotation'].to(device)\n            gt_t_absolute = batch['gt_translation'].to(device)\n            \n            # Map IDs\n            raw_obj_ids = batch['object_id'].tolist()\n            target_indices = torch.tensor(\n                [obj_id_to_idx[oid] for oid in raw_obj_ids], \n                dtype=torch.long, device=device\n            )\n\n            # Forward\n            pred_q, pred_t_res = model(points)\n\n            # Reconstruction\n            pred_t_abs = centroids + pred_t_res\n\n            # Loss\n            loss = criterion(\n                pred_q=pred_q, \n                pred_t=pred_t_abs, \n                gt_q=gt_rotations, \n                gt_t=gt_t_absolute, \n                class_indices=target_indices\n            )\n            \n            test_loss += loss.item()\n            val_pbar.set_postfix({'Val Loss': f\"{loss.item():.4f}\"})\n\n    # Calculate average validation loss\n    avg_test_loss = test_loss / len(test_loader)\n    test_losses.append(avg_test_loss)\n    \n    print(f\"üìä Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} m | Val Loss: {avg_test_loss:.4f} m\")\n\n    # ==========================================\n    # 3. CHECKPOINT SAVING\n    # ==========================================\n    if avg_test_loss < best_test_loss:\n        best_test_loss = avg_test_loss\n        checkpoint_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n        \n        # Handle DataParallel state_dict if necessary\n        model_state = model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict()\n        \n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model_state,\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_val_loss': best_test_loss,\n            'config': { \n                'num_points': 1024, # Useful for inference\n                'obj_map': obj_id_to_idx\n            }\n        }, checkpoint_path)\n        print(f\"‚úÖ New Record! Model saved with Loss: {best_test_loss:.4f} m\")\n    else:\n        print(f\"‚è≥ No improvement (Best: {best_test_loss:.4f} m)\")\n    \n    print(\"-\" * 60)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6666a692","outputId":"ab3b40a4-1a5d-4ce0-b172-17a7b1994075","trusted":true},"outputs":[],"execution_count":null},{"id":"d2823bc9","cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Create plots directory\n# plots_dir = \"plots\"\nplots_dir = checkpoint_dir\nos.makedirs(plots_dir, exist_ok=True)\n\n# Plot 1: Training vs Test Loss\nplt.figure(figsize=(10, 6))\nepochs_range = range(1, len(test_losses)+1)\nplt.plot(range(1, len(train_losses)+1), train_losses, 'b-o', label='Training Loss', linewidth=2, markersize=6)\nplt.plot(range(1, len(test_losses)+1), test_losses, 'r-s', label='Test Loss', linewidth=2, markersize=6)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('Training vs Test Loss', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nloss_plot_path = os.path.join(plots_dir, \"loss_comparison.png\")\nplt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\nprint(f\"‚úÖ Plot saved: {loss_plot_path}\")\nplt.show()\n\n# Plot 2: Only Training Loss\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(train_losses)+1), train_losses, 'b-o', linewidth=2, markersize=6)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Training Loss', fontsize=12)\nplt.title('Training Loss Over Epochs', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\ntrain_loss_path = os.path.join(plots_dir, \"training_loss.png\")\nplt.savefig(train_loss_path, dpi=300, bbox_inches='tight')\nprint(f\"‚úÖ Plot saved: {train_loss_path}\")\nplt.show()\n\n# Plot 3: Only Test Loss\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(test_losses)+1), test_losses, 'r-s', linewidth=2, markersize=6)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Test Loss', fontsize=12)\nplt.title('Test Loss Over Epochs', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.axhline(y=best_test_loss, color='g', linestyle='--', label=f'Best: {best_test_loss:.4f}', linewidth=2)\nplt.legend(fontsize=11)\nplt.tight_layout()\ntest_loss_path = os.path.join(plots_dir, \"test_loss.png\")\nplt.savefig(test_loss_path, dpi=300, bbox_inches='tight')\nprint(f\"‚úÖ Plot saved: {test_loss_path}\")\nplt.show()\n\nprint(f\"\\n‚úÖ All plots saved in '{plots_dir}' directory!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2025-12-20T10:53:58.626482Z","iopub.status.busy":"2025-12-20T10:53:58.626146Z","iopub.status.idle":"2025-12-20T10:54:00.560764Z","shell.execute_reply":"2025-12-20T10:54:00.560032Z","shell.execute_reply.started":"2025-12-20T10:53:58.626448Z"},"id":"d2823bc9","outputId":"ef287fb9-80b6-4626-8107-2d4e958acdbd","trusted":true},"outputs":[],"execution_count":null},{"id":"OEDg_DQr32yn","cell_type":"code","source":"# Save losses\nimport pickle\n\n\nlosses_dict = {\n    'train_losses': train_losses,\n    'test_losses': test_losses\n}\n\nlosses_path = os.path.join(checkpoint_dir, \"losses.pkl\")\nwith open(losses_path, 'wb') as f:\n    pickle.dump(losses_dict, f)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-20T10:54:12.535178Z","iopub.status.busy":"2025-12-20T10:54:12.534582Z","iopub.status.idle":"2025-12-20T10:54:12.539879Z","shell.execute_reply":"2025-12-20T10:54:12.539172Z","shell.execute_reply.started":"2025-12-20T10:54:12.535147Z"},"id":"OEDg_DQr32yn","trusted":true},"outputs":[],"execution_count":null},{"id":"da3dcf9c","cell_type":"markdown","source":"# Visualize samples","metadata":{"id":"da3dcf9c"}},{"id":"98a4a30b","cell_type":"code","source":"sample.keys()","metadata":{},"outputs":[],"execution_count":null},{"id":"cad99042","cell_type":"code","source":"import random\nimport cv2\nimport torch\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nfrom utils.projection_utils import setup_projection_utils, visualize_pose_comparison\n\n# ==========================================\n# 1. SETUP E CARICAMENTO MODELLO\n# ==========================================\n\n# Setup projection utils (assumiamo dataset_root sia definito)\nsetup_projection_utils(dataset_root)\n\n# Load best model\nbest_checkpoint_path = os.path.join(checkpoint_dir, \"best_model.pth\")\nif not os.path.exists(best_checkpoint_path):\n    raise FileNotFoundError(f\"Checkpoint non trovato: {best_checkpoint_path}\")\n\nprint(f\"üìÇ Caricamento checkpoint da: {best_checkpoint_path}\")\ncheckpoint = torch.load(best_checkpoint_path, map_location=device)\n\nstate_dict = checkpoint['model_state_dict']\n\n# Rimuovi il prefisso 'module.' se il modello era in DataParallel\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    name = k[7:] if k.startswith('module.') else k \n    new_state_dict[name] = v\n\n# Inizializza il modello (Assumiamo PointNetPoseModel sia importata)\n# model = PointNetPoseModel(num_points=1024).to(device) # Scommenta se devi instanziare\nmodel.load_state_dict(new_state_dict)\nmodel.eval()\n\nprint(f\"‚úÖ Modello caricato dall'epoca {checkpoint.get('epoch', '?')} con loss: {checkpoint.get('best_val_loss', '?'):.4f}\")\n\n# ==========================================\n# 2. SELEZIONE E PREPARAZIONE SAMPLE\n# ==========================================\n\n# Seleziona un indice casuale dal test dataset\nrandom_idx = random.randint(0, len(test_dataset) - 1)\nsample = test_dataset[random_idx]\n\nprint(f\"\\nüì∑ Visualizing Sample {random_idx}:\")\nprint(f\"   Object ID: {sample['object_id']}\")\n# Gestione robusta nel caso 'img_id' manchi (vecchi dataset)\nimg_id_display = sample.get('img_id', 'N/A')\nprint(f\"   Image ID: {img_id_display}\")\n\n# Recuperiamo l'immagine RGB originale per disegnare sopra\n# Nel dataset PointNet non carichiamo l'RGB nel __getitem__, quindi dobbiamo farlo a mano qui\n# Costruiamo il path usando le info nel sample o nel config\nif 'img_path' in sample:\n    img_path = sample['img_path']\nelse:\n    # Fallback: ricostruiamo il path se non √® nel sample\n    # (Assumendo struttura LineMod standard)\n    obj_id = sample['object_id']\n    img_id = sample['img_id']\n    img_path = os.path.join(dataset_root, 'data', f\"{obj_id:02d}\", 'rgb', f\"{img_id:04d}.png\")\n\nimage_bgr = cv2.imread(str(img_path))\nif image_bgr is None:\n    raise FileNotFoundError(f\"Impossibile caricare immagine da: {img_path}\")\nimage_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n\n# ==========================================\n# 3. INFERENZA POINTNET\n# ==========================================\n\n# Prepara i tensori (Aggiungi dimensione batch)\npoints = sample['points'].unsqueeze(0).to(device)       # (1, 3, N)\ncentroid = sample['centroid'].unsqueeze(0).to(device)   # (1, 3)\n\nwith torch.no_grad():\n    # Il modello restituisce rotazione E residuo traslazione\n    pred_q, pred_t_res = model(points)\n    \n    # Ricostruzione Traslazione Assoluta\n    # T_abs = Centroid + Residual\n    pred_trans_abs = centroid + pred_t_res\n\n# Converti in numpy per visualizzazione\npred_rotation = pred_q[0].cpu().numpy()\npred_translation = pred_trans_abs[0].cpu().numpy()\n\n# Ground Truth\ngt_rotation = sample['rotation'].numpy()\ngt_translation = sample['gt_translation'].numpy() # O sample['translation']\ncam_K = sample['cam_K'].numpy()\n\nprint(f\"\\nüìä Ground Truth vs Prediction:\")\nprint(f\"   GT Rotation:   {gt_rotation}\")\nprint(f\"   Pred Rotation: {pred_rotation}\")\nprint(f\"   GT Trans (m):   {gt_translation}\")\nprint(f\"   Pred Trans (m): {pred_translation}\")\n\n# Calcola errore distanza (solo per curiosit√†)\ndist_error = np.linalg.norm(gt_translation - pred_translation)\nprint(f\"   Translation Error: {dist_error*100:.2f} cm\")\n\n# ==========================================\n# 4. VISUALIZZAZIONE\n# ==========================================\n\n# Visualizza confronto pose\n# Nota: La funzione visualize_pose_comparison si aspetta un'immagine RGB (numpy)\nimg_vis = visualize_pose_comparison(\n    image_rgb,\n    object_id=sample['object_id'],\n    cam_K=cam_K,\n    gt_rotation=gt_rotation,\n    gt_translation=gt_translation,\n    pred_rotation=pred_rotation,\n    pred_translation=pred_translation  # <-- ORA USIAMO LA TRASLAZIONE PREDETTA!\n)\n\n# Plot con Matplotlib\nfig, ax = plt.subplots(1, 1, figsize=(14, 8))\n# visualize_pose_comparison ritorna RGB se gli passi RGB, quindi ok\nax.imshow(img_vis)\nax.axis('off')\nax.set_title(f\"PointNet Pose - Obj {sample['object_id']} (Err: {dist_error*100:.1f}cm)\", fontsize=16, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n‚úÖ Visualizzazione completata!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2025-12-20T11:01:38.744094Z","iopub.status.busy":"2025-12-20T11:01:38.743779Z","iopub.status.idle":"2025-12-20T11:01:39.607462Z","shell.execute_reply":"2025-12-20T11:01:39.606597Z","shell.execute_reply.started":"2025-12-20T11:01:38.744066Z"},"id":"cad99042","outputId":"8870604a-00eb-4a86-ef5d-8708e035b204","trusted":true},"outputs":[],"execution_count":null},{"id":"2cb5fe35-d2eb-4efc-aa76-5c0eb70a98f6","cell_type":"code","source":"!pip install trimesh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T21:00:47.252583Z","iopub.execute_input":"2025-12-20T21:00:47.253167Z","iopub.status.idle":"2025-12-20T21:00:51.158116Z","shell.execute_reply.started":"2025-12-20T21:00:47.253137Z","shell.execute_reply":"2025-12-20T21:00:51.157388Z"}},"outputs":[{"name":"stdout","text":"Collecting trimesh\n  Downloading trimesh-4.10.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from trimesh) (2.0.2)\nDownloading trimesh-4.10.1-py3-none-any.whl (737 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m737.0/737.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trimesh\nSuccessfully installed trimesh-4.10.1\n","output_type":"stream"}],"execution_count":22},{"id":"c862c6a3-6c12-4c90-b228-9ea1a1a47518","cell_type":"code","source":"import torch\nimport numpy as np\nimport os\nimport trimesh\nimport pandas as pd\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\nfrom metrics.ADD_metric import compute_ADD_metric_quaternion, compute_ADDs_metric_quaternion\n# Ensure you import the correct PointNet model class here\nfrom src.pose_pointnet.model import PointNetPoseModel \n\n# ==========================================\n# 1. LOAD DATA AND DIAMETERS\n# ==========================================\ndef load_models_info(models_dir, obj_ids, num_points=1000):\n    \"\"\"\n    Loads sampled points and calculates the DIAMETER of each object.\n    (This function remains unchanged as it works on .ply files).\n    \"\"\"\n    point_cache = {}\n    diameters = {}\n    \n    unique_ids = list(set(obj_ids))\n    print(f\"‚è≥ Loading info for {len(unique_ids)} models...\")\n    \n    for oid in tqdm(unique_ids, desc=\"Mesh Analysis\"):\n        filename = f\"obj_{int(oid):02d}.ply\"\n        path = os.path.join(models_dir, filename)\n        \n        if os.path.exists(path):\n            mesh = trimesh.load(path)\n            # 1. Sample points for ADD metric calculation\n            points, _ = trimesh.sample.sample_surface(mesh, num_points)\n            point_cache[oid] = points / 1000.0  # Convert mm to Meters\n            \n            # 2. Diameter Calculation (Max distance in the mesh)\n            extents = mesh.extents / 1000.0  # Meters\n            diameter = np.linalg.norm(extents)\n            diameters[oid] = diameter\n        else:\n            print(f\"‚ö†Ô∏è Missing model file: {path}\")\n            \n    return point_cache, diameters\n\n# ==========================================\n# 2. COMPREHENSIVE EVALUATION (POINTNET VERSION)\n# ==========================================\ndef evaluate_comprehensive(model, dataloader, device, models_dir, output_csv=\"evaluation_results.csv\"):\n    \"\"\"\n    Evaluates the PointNet model using ADD and ADD-S metrics.\n    \"\"\"\n    model.eval()\n    \n    # --- MAPPING ID TO NAMES (LineMOD Standard) ---\n    id_to_name = {\n        1: 'ape', 2: 'benchvise', 4: 'camera', 5: 'can', 6: 'cat',\n        8: 'driller', 9: 'duck', 10: 'eggbox', 11: 'glue',\n        12: 'holepuncher', 13: 'iron', 14: 'lamp', 15: 'phone'\n    }\n\n    # Define IDs to evaluate\n    all_obj_ids = list(id_to_name.keys())\n    \n    # Load mesh data (Points and Diameters)\n    points_dict, diameters_dict = load_models_info(models_dir, all_obj_ids)\n    \n    # Data Structures for logging\n    errors_dict = defaultdict(list)\n    accuracy_stats = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n\n    # Objects requiring ADD-S (Symmetric objects)\n    SYMMETRIC_OBJECTS = [10, 11]  # Eggbox, Glue\n    \n    print(\"\\nüöÄ Starting Comprehensive Benchmark (ADD Error + ADD-0.1d Accuracy)...\")\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            # --- UPDATED INPUTS FOR POINTNET ---\n            # We load points and centroids, not images\n            points = batch['points'].to(device)           # (B, 3, N)\n            centroids = batch['centroid'].to(device)      # (B, 3)\n            \n            gt_quats = batch['rotation'].to(device)       # (B, 4)\n            gt_trans = batch['gt_translation'].to(device) # (B, 3) - Absolute GT\n            obj_ids = batch['object_id']\n            \n            # --- FORWARD PASS ---\n            # Predict Rotation and Residual Translation\n            pred_quats, pred_t_res = model(points)\n            \n            # --- RECONSTRUCT ABSOLUTE TRANSLATION ---\n            # Abs_Trans = Centroid + Residual\n            pred_trans_abs = centroids + pred_t_res\n            \n            batch_size = points.shape[0]\n            for i in range(batch_size):\n                curr_id = int(obj_ids[i])\n                if curr_id not in points_dict: continue\n\n                # Select Metric: ADD-S for symmetric, ADD for others\n                metric = compute_ADDs_metric_quaternion if curr_id in SYMMETRIC_OBJECTS else compute_ADD_metric_quaternion\n                \n                # --- CALCULATE ERROR ---\n                # We pass the PREDICTED translation (pred_trans_abs), not the GT one!\n                # This evaluates the full 6D pose (Rot + Trans).\n                add_error = metric(\n                    model_points=points_dict[curr_id],\n                    gt_quat=gt_quats[i].cpu().numpy(),\n                    gt_translation=gt_trans[i].cpu().numpy(),\n                    pred_quat=pred_quats[i].cpu().numpy(),\n                    pred_translation=pred_trans_abs[i].cpu().numpy() \n                )\n                \n                # Store absolute error\n                errors_dict[curr_id].append(add_error)\n                \n                # Calculate Accuracy (Threshold = 10% of diameter)\n                threshold = diameters_dict[curr_id] * 0.1\n                accuracy_stats[curr_id][\"total\"] += 1\n                if add_error < threshold:\n                    accuracy_stats[curr_id][\"correct\"] += 1\n\n    # ==========================================\n    # 3. PANDAS REPORT GENERATION\n    # ==========================================\n    results_data = []\n    \n    total_acc_correct = 0\n    total_acc_count = 0\n    total_errors = []\n\n    sorted_ids = sorted(errors_dict.keys())\n    \n    for oid in sorted_ids:\n        # Error stats\n        mean_err_m = np.mean(errors_dict[oid])\n        mean_err_cm = mean_err_m * 100.0\n        total_errors.extend(errors_dict[oid])\n        \n        # Accuracy stats\n        stats = accuracy_stats[oid]\n        acc_perc = (stats[\"correct\"] / stats[\"total\"]) * 100.0 if stats[\"total\"] > 0 else 0.0\n        \n        total_acc_correct += stats[\"correct\"]\n        total_acc_count += stats[\"total\"]\n        \n        diam_cm = diameters_dict[oid] * 100.0\n        \n        # Get Class Name\n        class_name = id_to_name.get(oid, \"Unknown\")\n\n        # Append to list\n        results_data.append({\n            \"Object ID\": oid,\n            \"Class Name\": class_name,\n            \"Diameter (cm)\": round(diam_cm, 2),\n            \"Mean ADD Error (cm)\": round(mean_err_cm, 2),\n            \"Accuracy (%)\": round(acc_perc, 2),\n            \"Samples\": stats['total']\n        })\n\n    # Create DataFrame\n    df = pd.DataFrame(results_data)\n\n    # Calculate Global Stats\n    global_mean_error_cm = np.mean(total_errors) * 100.0 if total_errors else 0.0\n    global_accuracy = (total_acc_correct / total_acc_count * 100.0) if total_acc_count > 0 else 0.0\n\n    # Add Global Row\n    global_row = pd.DataFrame([{\n        \"Object ID\": \"GLOBAL\",\n        \"Class Name\": \"ALL\", \n        \"Diameter (cm)\": \"-\",\n        \"Mean ADD Error (cm)\": round(global_mean_error_cm, 2),\n        \"Accuracy (%)\": round(global_accuracy, 2),\n        \"Samples\": total_acc_count\n    }])\n    \n    df = pd.concat([df, global_row], ignore_index=True)\n\n    # Print and Save\n    print(\"\\n\" + \"=\"*80)\n    print(\"FINAL EVALUATION REPORT (POINTNET)\")\n    print(\"=\"*80)\n    pd.set_option('display.max_rows', None)\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.width', 1000)\n    \n    print(df.to_string(index=False))\n    print(\"=\"*80)\n    \n    df.to_csv(output_csv, index=False)\n    print(f\"‚úÖ Results saved to {output_csv}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-20T21:00:52.670216Z","iopub.execute_input":"2025-12-20T21:00:52.670881Z","iopub.status.idle":"2025-12-20T21:00:54.045322Z","shell.execute_reply.started":"2025-12-20T21:00:52.670847Z","shell.execute_reply":"2025-12-20T21:00:54.044717Z"},"trusted":true},"outputs":[],"execution_count":23},{"id":"6fcbdf81","cell_type":"code","source":"from collections import OrderedDict\n# --- USAGE ---\n\n# 1. Define Paths\nMODELS_ROOT = \"/kaggle/input/line-mode/Linemod_preprocessed/models\"\n# MODELS_ROOT = '../../Linemod_preprocessed_small/models'\ncheckpoint_path = checkpoint_dir + \"/best_model.pth\"\n\n# 2. Load Checkpoint\nprint(f\"üìÇ Loading checkpoint from: {checkpoint_path}\")\ndata = torch.load(checkpoint_path, map_location=device)\nstate_dict = data['model_state_dict']\n\n# 3. Clean State Dict (Remove 'module.' prefix from DataParallel)\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    name = k[7:] if k.startswith('module.') else k \n    new_state_dict[name] = v\n\n# 4. Initialize and Load Model\ntest_model = PointNetPoseModel()\ntest_model.load_state_dict(new_state_dict)\ntest_model.to(device)\n\n# 5. Run Evaluation\nevaluate_comprehensive(\n    test_model, \n    test_loader, \n    device, \n    MODELS_ROOT, \n    output_csv=checkpoint_dir + '/linemod_results.csv'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T21:00:56.282951Z","iopub.execute_input":"2025-12-20T21:00:56.283480Z","iopub.status.idle":"2025-12-20T21:01:07.319471Z","shell.execute_reply.started":"2025-12-20T21:00:56.283449Z","shell.execute_reply":"2025-12-20T21:01:07.318535Z"}},"outputs":[{"name":"stdout","text":"üìÇ Loading checkpoint from: /kaggle/working/POINTNET_20251220_204610/best_model.pth\n‚è≥ Loading info for 13 models...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Mesh Analysis:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4863a67651e439e91c8dc8a5af11b44"}},"metadata":{}},{"name":"stdout","text":"\nüöÄ Starting Comprehensive Benchmark (ADD Error + ADD-0.1d Accuracy)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91e842028cd146e59b33d7fe4fec4423"}},"metadata":{}},{"name":"stdout","text":"\n================================================================================\nFINAL EVALUATION REPORT (POINTNET)\n================================================================================\nObject ID  Class Name Diameter (cm)  Mean ADD Error (cm)  Accuracy (%)  Samples\n        1         ape         14.21                 2.12         41.53      248\n        2   benchvise         33.09                 2.97         72.02      243\n        4      camera         22.19                 2.92         51.45      241\n        5         can         28.42                 2.77         62.92      240\n        6         cat         18.59                 1.94         61.02      236\n        8     driller         31.88                 3.62         64.29      238\n        9        duck         15.57                 3.10         25.50      251\n       10      eggbox          19.7                 0.98        100.00      251\n       11        glue         19.31                 0.85         99.59      244\n       12 holepuncher         17.38                 2.70         47.58      248\n       13        iron         31.72                 8.91         17.75      231\n       14        lamp         31.66                 4.96         37.80      246\n       15       phone         25.43                 2.25         67.47      249\n   GLOBAL         ALL             -                 3.06         57.74     3166\n================================================================================\n‚úÖ Results saved to /kaggle/working/POINTNET_20251220_204610/linemod_results.csv\n","output_type":"stream"}],"execution_count":24},{"id":"d10409e3","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}