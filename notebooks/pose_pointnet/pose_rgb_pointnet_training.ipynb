{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad8daca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dad8daca",
        "outputId": "1e1ddb79-4ff6-4bc1-df0f-4482b718216f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone or pull part\n",
        "repo_url = \"https://github.com/fraco03/6D_pose.git\"\n",
        "repo_dir = \"/content/6D_pose\"\n",
        "branch = \"pose_rgb\"\n",
        "\n",
        "if not os.path.exists(repo_dir):\n",
        "    !git clone -b {branch} {repo_url}\n",
        "    print(f\"Cloned {repo_url}\")\n",
        "else:\n",
        "    %cd {repo_dir}\n",
        "    !git fetch origin\n",
        "    !git checkout {branch}\n",
        "    !git reset --hard origin/{branch}\n",
        "    %cd ..\n",
        "    print(f\"Updated {repo_url}\")\n",
        "\n",
        "if repo_dir not in sys.path:\n",
        "    sys.path.insert(0, repo_dir)\n",
        "\n",
        "%cd 6D_pose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SQ3b58fTYu_g",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SQ3b58fTYu_g",
        "outputId": "b823c960-2b24-4c6d-c92c-86ebe0a4281a"
      },
      "outputs": [],
      "source": [
        "%cd ..\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1qQ8ZjUI6QauzFsiF8EpaaI2nKFWna_kQ/view?usp=sharing -O Linemod_preprocessed.zip\n",
        "!unzip Linemod_preprocessed.zip\n",
        "%cd 6D_pose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "209858ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "209858ea",
        "outputId": "9aa2a4a7-b18c-441f-a740-288747fc9cf6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from utils.load_data import mount_drive\n",
        "\n",
        "mount_drive()\n",
        "\n",
        "# dataset_root = \"/content/drive/MyDrive/Linemod_preprocessed\"\n",
        "dataset_root = \"/content/Linemod_preprocessed\"\n",
        "print(f\"\\nâœ… Setup complete!\")\n",
        "print(f\"ðŸ“ Dataset path: {dataset_root}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf7c73b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf7c73b5",
        "outputId": "05d51540-96d4-4173-ecd9-be5232845cab"
      },
      "outputs": [],
      "source": [
        "!pip install plyfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eae81c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eae81c8",
        "outputId": "340bd5ee-87f4-4e5d-b38e-b482c7c42ca2"
      },
      "outputs": [],
      "source": [
        "from src.pose_pointnet.pointcloud_dataset import LineModPointCloudDataset\n",
        "from src.pose_pointnet.pointnet_model import PointNetPose\n",
        "from src.pose_rgb.loss import AutomaticWeightedLoss\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"ðŸ”¥ Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff242eb0",
      "metadata": {
        "id": "ff242eb0"
      },
      "source": [
        "## ðŸŽ¯ Rotation-Only Approach\n",
        "\n",
        "**Simple and Clean:**\n",
        "- **Model**: Predicts ONLY rotation (quaternion)\n",
        "- **Loss**: Simple `RotationLoss` - no complex multi-task balancing needed\n",
        "- **Translation**: Computed from depth + bbox using pinhole geometry (not learned)\n",
        "\n",
        "**Why this works:**\n",
        "- Depth already gives us Z â†’ no need to learn it\n",
        "- Pinhole geometry is deterministic â†’ no need to learn X, Y from point cloud\n",
        "- PointNet focuses on what it's good at: extracting shape features for rotation\n",
        "\n",
        "**Math:**\n",
        "- Depth at bbox center: $Z = \\text{depth}[c_x, c_y]$\n",
        "- Back-projection: $X = \\frac{(c_x - c_x^{intr}) \\cdot Z}{f_x}$, $Y = \\frac{(c_y - c_y^{intr}) \\cdot Z}{f_y}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1ac005",
      "metadata": {
        "id": "0f1ac005"
      },
      "source": [
        "## âš¡ Performance Optimizations\n",
        "\n",
        "**Optimizations applied to accelerate training:**\n",
        "\n",
        "1. **Point reduction**: 1024 â†’ 512 points per point cloud (~2x speed-up).\n",
        "2. **Cached YAML files**: `linemod_config` caches `info.yml` and `gt.yml` instead of opening them for every iteration (~10-20x speed-up!).\n",
        "    - **Critical on Google Drive**: I/O latency is extremely high, making caching essential.\n",
        "3. **Torch sampling**: Used `torch.randperm` instead of `np.random.choice` (~1.5x speed-up).\n",
        "4. **Mixed precision**: Automatic FP16/FP32 training with `torch.cuda.amp` (~1.5x speed-up).\n",
        "\n",
        "**Estimated total speed-up: ~20-30x** compared to the initial version! ðŸš€\n",
        "\n",
        "**Note**: The first batch may take longer to load and cache all YAML files; subsequent iterations will be significantly faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9722028",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9722028",
        "outputId": "3711693f-50e9-4cd9-fcee-0583b016204f"
      },
      "outputs": [],
      "source": [
        "# Crea dataset con point clouds\n",
        "# Nota: Prediciamo SOLO rotazione, la traslazione viene ricavata da depth + bbox usando pinhole geometry\n",
        "\n",
        "train_dataset = LineModPointCloudDataset(\n",
        "    root_dir=dataset_root,\n",
        "    split='train',\n",
        "    num_points=512,\n",
        "    use_rgb=True\n",
        ")\n",
        "\n",
        "test_dataset = LineModPointCloudDataset(\n",
        "    root_dir=dataset_root,\n",
        "    split='test',\n",
        "    num_points=512,\n",
        "    use_rgb=True\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "befa5059",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "befa5059",
        "outputId": "65814c45-5b0d-45e7-ff6f-f4fbeeb7398c"
      },
      "outputs": [],
      "source": [
        "# Visualizza un sample\n",
        "sample = train_dataset[0]\n",
        "\n",
        "print(\"Sample keys:\", sample.keys())\n",
        "print(f\"Point cloud shape: {sample['point_cloud'].shape}\")  # (512, 6)\n",
        "print(f\"Rotation shape: {sample['rotation'].shape}\")        # (4,) - TENSOR\n",
        "print(f\"Depth Z: {sample['depth_z']:.4f} m\")\n",
        "print(f\"\\nRotation (quat, tensor): {sample['rotation']}\")\n",
        "print(f\"Bbox info (tensor): {sample['bbox_info']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c441ebe0",
      "metadata": {
        "id": "c441ebe0"
      },
      "outputs": [],
      "source": [
        "# DataLoaders - Ottimizzati per performance\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,  # Aumentato da 32 per migliore GPU utilization\n",
        "    shuffle=True,\n",
        "    num_workers=2,  # Aumentato da 2\n",
        "    pin_memory=True  # Velocizza transfer CPU->GPU\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54dbb9a9",
      "metadata": {
        "id": "54dbb9a9"
      },
      "outputs": [],
      "source": [
        "# Import visualization utilities\n",
        "from utils.projection_utils import visualize_pose_comparison\n",
        "from utils.linemod_config import get_linemod_config\n",
        "from utils.projection_utils import setup_projection_utils\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Config per caricare modelli 3D\n",
        "config = get_linemod_config(dataset_root)\n",
        "setup_projection_utils(dataset_root)\n",
        "\n",
        "def compute_translation_from_depth(bbox_info, depth_z, cam_K, img_shape):\n",
        "    \"\"\"\n",
        "    Compute 3D translation using pinhole geometry.\n",
        "    bbox_info: tensor or numpy array\n",
        "    depth_z: scalar (tensor or float)\n",
        "    cam_K: tensor or numpy array\n",
        "    \"\"\"\n",
        "    H, W = img_shape\n",
        "    \n",
        "    # Convert to numpy if needed\n",
        "    if isinstance(bbox_info, torch.Tensor):\n",
        "        bbox_info = bbox_info.numpy()\n",
        "    if isinstance(cam_K, torch.Tensor):\n",
        "        cam_K = cam_K.numpy()\n",
        "    if isinstance(depth_z, torch.Tensor):\n",
        "        depth_z = depth_z.item()\n",
        "    \n",
        "    cx_norm, cy_norm = bbox_info[0], bbox_info[1]\n",
        "    \n",
        "    # Convert normalized to pixel coordinates\n",
        "    cx_pixel = cx_norm * W\n",
        "    cy_pixel = cy_norm * H\n",
        "    \n",
        "    # Back-project using pinhole model\n",
        "    fx = cam_K[0, 0]\n",
        "    fy = cam_K[1, 1]\n",
        "    cx_intr = cam_K[0, 2]\n",
        "    cy_intr = cam_K[1, 2]\n",
        "    \n",
        "    Z = depth_z\n",
        "    X = (cx_pixel - cx_intr) * Z / fx\n",
        "    Y = (cy_pixel - cy_intr) * Z / fy\n",
        "    \n",
        "    return np.array([X, Y, Z])\n",
        "\n",
        "def visualize_training_sample(model, dataset, dataset_root, checkpoint_dir, epoch, device='cuda'):\n",
        "    \"\"\"\n",
        "    Visualizza una predizione casuale e la salva.\n",
        "    \"\"\"\n",
        "    import shutil\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Sample casuale\n",
        "    idx = np.random.randint(0, len(dataset))\n",
        "    sample = dataset[idx]\n",
        "\n",
        "    obj_id = sample['object_id']\n",
        "    img_id = sample['img_id']\n",
        "\n",
        "    # Carica immagine originale\n",
        "    img_path = Path(dataset_root) / \"data\" / f\"{obj_id:02d}\" / \"rgb\" / f\"{img_id:04d}.png\"\n",
        "    image = cv2.imread(str(img_path))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    H, W = image.shape[:2]\n",
        "\n",
        "    # Prepare input\n",
        "    point_cloud = sample['point_cloud'].unsqueeze(0).to(device)\n",
        "    bbox_info = sample['bbox_info'].unsqueeze(0).to(device)\n",
        "\n",
        "    # Inference - ONLY rotation\n",
        "    with torch.no_grad():\n",
        "        pred_rot = model(point_cloud, bbox_info)\n",
        "\n",
        "    # Convert to numpy\n",
        "    pred_rot = pred_rot.squeeze(0).cpu().numpy()\n",
        "    gt_rot = sample['rotation'].cpu().numpy()\n",
        "    cam_K = sample['cam_K'].numpy()\n",
        "    \n",
        "    # Compute translations from depth + bbox (same for both gt and pred)\n",
        "    trans = compute_translation_from_depth(sample['bbox_info'], sample['depth_z'], cam_K, (H, W))\n",
        "\n",
        "    # Visualizza (rotation is different, translation is same)\n",
        "    img_vis = visualize_pose_comparison(\n",
        "        image, obj_id, cam_K,\n",
        "        gt_rot, trans,\n",
        "        pred_rot, trans\n",
        "    )\n",
        "\n",
        "    # Calcola errore di rotazione\n",
        "    dot_product = np.abs(np.dot(pred_rot, gt_rot))\n",
        "    dot_product = np.clip(dot_product, -1.0, 1.0)\n",
        "    angle_error = 2 * np.arccos(dot_product) * 180 / np.pi\n",
        "\n",
        "    # Salva\n",
        "    tmp_path = f\"/tmp/vis_epoch_{epoch:03d}_obj_{obj_id:02d}.png\"\n",
        "    fig = plt.figure(figsize=(12, 10))\n",
        "    plt.imshow(img_vis)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Object {obj_id} | Rotation Error: {angle_error:.2f}Â°\")\n",
        "    plt.savefig(tmp_path, bbox_inches='tight', dpi=100)\n",
        "    plt.close()\n",
        "\n",
        "    # Copia su Drive\n",
        "    vis_dir = Path(checkpoint_dir) / \"visualizations\"\n",
        "    vis_dir.mkdir(exist_ok=True)\n",
        "    drive_path = vis_dir / f\"epoch_{epoch:03d}_obj_{obj_id:02d}.png\"\n",
        "    shutil.copy(tmp_path, str(drive_path))\n",
        "\n",
        "    print(f\"âœ… Saved: epoch {epoch} | Rotation Error: {angle_error:.2f}Â°\")\n",
        "\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcaf4309",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcaf4309",
        "outputId": "3621dc62-0a13-4517-ac8e-0e5e8a4cc5d1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from itertools import islice\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.amp import autocast, GradScaler\n",
        "from src.pose_pointnet.pointnet_model import PointNetPose\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# HYPERPARAMETERS\n",
        "# ==========================================\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 50\n",
        "USE_MIXED_PRECISION = True\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "CHECKPOINT_DIR = f'/content/drive/MyDrive/runs/pointnet_{timestamp}'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Initialize PointNet Model - ROTATION ONLY\n",
        "model = PointNetPose(input_channels=6, use_batch_norm=True).to(DEVICE)\n",
        "\n",
        "# Loss - SIMPLE: Only Rotation Loss\n",
        "from src.pose_pointnet.loss import PoseMatchingLoss\n",
        "criterion = PoseMatchingLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Mixed Precision Scaler\n",
        "scaler = GradScaler('cuda') if USE_MIXED_PRECISION else None\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "\n",
        "print(f\"\\nðŸ”¥ STARTING POINTNET ROTATION-ONLY TRAINING on {DEVICE}...\")\n",
        "print(f\"ðŸ“ Checkpoints: {CHECKPOINT_DIR}\")\n",
        "print(f\"âš™ï¸  Loss: RotationLoss (simple quaternion loss)\")\n",
        "print(f\"âš¡ Mixed Precision: {USE_MIXED_PRECISION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c66a224",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Rotation-Only Model Architecture\n",
        "\n",
        "**Key Insight**: With depth available, we only need to predict **rotation**. Translation is computed directly using pinhole geometry:\n",
        "\n",
        "$$X = \\frac{(c_x - c_x^{intr}) \\cdot Z}{f_x}, \\quad Y = \\frac{(c_y - c_y^{intr}) \\cdot Z}{f_y}, \\quad Z = \\text{depth}[c_x, c_y]$$\n",
        "\n",
        "**Advantages:**\n",
        "- Simpler model (no translation head)\n",
        "- Faster training (only rotation loss)\n",
        "- More stable (depth provides ground truth Z)\n",
        "- Cleaner optimization (one prediction target instead of two)\n",
        "\n",
        "**Dataset returns:**\n",
        "- `point_cloud`: Local point cloud (NÃ—3 or NÃ—6 with RGB)\n",
        "- `rotation`: Target quaternion\n",
        "- `depth_z`: Z coordinate from depth at bbox center (for inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659ef393",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "659ef393",
        "outputId": "b35276c1-4a0d-43b6-ce6e-a794a34182c4"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# TRAINING LOOP - SIMPLE ROTATION LOSS\n",
        "# ==========================================\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    # --- TRAIN PHASE ---\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        # Move to device\n",
        "        point_clouds = batch['point_cloud'].to(DEVICE, non_blocking=True)  # (B, N, 6)\n",
        "        bbox_info = batch['bbox_info'].to(DEVICE, non_blocking=True)       # (B, 4)\n",
        "        gt_rot = batch['rotation'].to(DEVICE, non_blocking=True)           # (B, 4)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        if USE_MIXED_PRECISION:\n",
        "            with autocast('cuda'):\n",
        "                pred_rot = model(point_clouds, bbox_info)\n",
        "                loss = criterion(pred_rot, gt_rot)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            pred_rot = model(point_clouds, bbox_info)\n",
        "            loss = criterion(pred_rot, gt_rot)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "        pbar.set_postfix({'Loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_train_loss = running_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # --- VALIDATION PHASE ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    val_batches_limit = 50\n",
        "    count_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_iterator = islice(test_loader, val_batches_limit)\n",
        "        val_pbar = tqdm(val_iterator, total=val_batches_limit, desc=\"Validating\")\n",
        "\n",
        "        for batch in val_pbar:\n",
        "            point_clouds = batch['point_cloud'].to(DEVICE, non_blocking=True)\n",
        "            bbox_info = batch['bbox_info'].to(DEVICE, non_blocking=True)\n",
        "            gt_rot = batch['rotation'].to(DEVICE, non_blocking=True)\n",
        "\n",
        "            if USE_MIXED_PRECISION:\n",
        "                with autocast('cuda'):\n",
        "                    pred_rot = model(point_clouds, bbox_info)\n",
        "                    loss = criterion(pred_rot, gt_rot)\n",
        "            else:\n",
        "                pred_rot = model(point_clouds, bbox_info)\n",
        "                loss = criterion(pred_rot, gt_rot)\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "            count_batches += 1\n",
        "\n",
        "    avg_val_loss = running_val_loss / count_batches if count_batches > 0 else 0.0\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    # --- REPORT & SAVE ---\n",
        "    print(f\"ðŸ“Š Epoch {epoch+1}: Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f}\")\n",
        "\n",
        "    # --- VISUALIZE RANDOM SAMPLE ---\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      print(f\"ðŸŽ¨ Visualizing random validation sample...\")\n",
        "      visualize_training_sample(model, test_dataset, dataset_root, CHECKPOINT_DIR, epoch, DEVICE)\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_epoch = epoch + 1\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'val_loss': best_val_loss\n",
        "        }, os.path.join(CHECKPOINT_DIR, \"best_model.pth\"))\n",
        "\n",
        "        print(f\"ðŸ† New Best Model! (Loss: {best_val_loss:.4f})\")\n",
        "\n",
        "    # Save last checkpoint\n",
        "    if (epoch + 1) == NUM_EPOCHS:\n",
        "        torch.save({\n",
        "            'epoch': epoch+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "        }, os.path.join(CHECKPOINT_DIR, f\"checkpoint_ep{epoch+1}.pth\"))\n",
        "\n",
        "print(\"\\nðŸŽ‰ TRAINING COMPLETE!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdbd1b1a",
      "metadata": {
        "id": "cdbd1b1a"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss', marker='o', alpha=0.7)\n",
        "plt.plot(val_losses, label='Validation Loss', marker='s', alpha=0.7)\n",
        "if best_epoch > 0:\n",
        "    plt.axvline(x=best_epoch-1, color='r', linestyle='--', alpha=0.5,\n",
        "                label=f'Best Epoch ({best_epoch})')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('PointNet Training History')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_losses, label='Train Loss', marker='o', alpha=0.7)\n",
        "plt.plot(val_losses, label='Validation Loss', marker='s', alpha=0.7)\n",
        "if best_epoch > 0:\n",
        "    plt.axvline(x=best_epoch-1, color='r', linestyle='--', alpha=0.5,\n",
        "                label=f'Best Epoch ({best_epoch})')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (log scale)')\n",
        "plt.yscale('log')\n",
        "plt.title('PointNet Training History (Log)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(CHECKPOINT_DIR, 'training_history.png'), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nðŸ“Š Training Statistics:\")\n",
        "print(f\"   Best epoch: {best_epoch}\")\n",
        "print(f\"   Best val loss: {best_val_loss:.6f}\")\n",
        "print(f\"   Final train loss: {train_losses[-1]:.6f}\")\n",
        "\n",
        "# Save history\n",
        "history = {\n",
        "    'train_losses': [float(x) for x in train_losses],\n",
        "    'val_losses': [float(x) for x in val_losses],\n",
        "    'best_epoch': int(best_epoch),\n",
        "    'best_val_loss': float(best_val_loss),\n",
        "    'timestamp': timestamp\n",
        "}\n",
        "\n",
        "with open(os.path.join(CHECKPOINT_DIR, 'history.json'), 'w') as f:\n",
        "    json.dump(history, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc8bf1c",
      "metadata": {
        "id": "6cc8bf1c"
      },
      "source": [
        "## Visualize Point Cloud Sample\n",
        "\n",
        "Visualizing point cloud from random point in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e081a4e7",
      "metadata": {
        "id": "e081a4e7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Choose sample\n",
        "sample = train_dataset[100]\n",
        "pc = sample['point_cloud'].numpy()  # (512, 6) [x, y, z, r, g, b]\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot 3D points with RGB colors\n",
        "ax.scatter(pc[:, 0], pc[:, 1], pc[:, 2],\n",
        "           c=pc[:, 3:6], s=2, alpha=0.6)\n",
        "\n",
        "ax.set_xlabel('X (m)')\n",
        "ax.set_ylabel('Y (m)')\n",
        "ax.set_zlabel('Z (m)')\n",
        "ax.set_title(f'Point Cloud - Object {sample[\"object_id\"]}')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Object ID: {sample['object_id']}\")\n",
        "print(f\"Rotation (quat, tensor): {sample['rotation']}\")\n",
        "print(f\"GT Translation (m, tensor): {sample['gt_translation']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xBzdBr4w9w7V",
      "metadata": {
        "id": "xBzdBr4w9w7V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
